{"type":"solution","filename":"provisioning_platforms/index.asciidoc","title":"Provisioning Platforms","breadcrumbs":[],"text":"//Category=Provisioning\n//Platform=Azure\n//Maturity level=Complete\n\n:toc: macro\ntoc::[]\n:idprefix:\n:idseparator: -\n\n== Provisioning Platforms\n\n=== Azure\n\n==== Overview\n\nThis chapter lists major features/ concrete services for provisioning of the Azure platform. This architecture pattern builds on the general problem description for monitoring. The picture below summarizes major services and concepts that are discussed in detail in the next chpater.\n\nimage::provisioning_azure.png[Provisioning Azure Overview, width=966, height=526]\n\n==== Automation code\n\n===== Modelling Environments\n\nAzure provides the following structural elements to model an environment:\n\n* *Resource groups:* Smallest possible container\n* *Subscriptions:* One subscription can contain many resource groups. With a subscription discounts for dev/ test environments are possible.\n* *Management groups:* One management group can contain many subscriptions. They can be used enforce policies across subscriptions if environments share common characteristics.\n\nAn environment can be linked to another environment. Linking an environment to multiple environment is beneficial for addressing cross concerns such as monitoring (Similar to Hub/ Spoke topology in networks).\n\n===== Pipeline Implementation\n\nThe programming approach can be either UI driven or *programmatic*. Pipeline programming languages such as YAML structure the actions to be performed by the pipeline and provide basic mechansims such as downloading code from the repo, parameter handling, stating triggers and triggering other programming languages. These other languages are then used to setup infrastructure such as terraform or deploying application code.\n\nAzure allows to https://docs.microsoft.com/en-us/azure/devops/pipelines/build/triggers?view=azure-devops[*trigger* pipelines] upon:\n\n* a push to repo\n* a pull request to repo\n* a schedule\n* a pipeline completion\n\nThe platform allows to *pass parameters* by various mechanisms to pipelines(Explicit per user input, programmatically). Parameters can be passed by group identifier or explicitly as key value pairs. Complex structured objects as known from object programming languages are not directly possible (Require parsing of files with object structure). Parametrization might be constrained by the used service in certain areas.\n\nThe platform provides *support for quality gates* as follows:\n\n* Static code analysis\n+\nMicrosoft does not provide own tools for static code analysis but allows integration of others.\n* Automated tests (Unit, Integration, End-To-End)\n+\nMicrosoft provides services that include test management e.g. creating test suites with test cases and getting an overview about the results.\n* Approval\n+\nAzure services support approval for a certain environments and enforcing pull requests as quality gates.\n\nThe Azure platform provides the following basic options to *store* automation code:\n\n* Services that provide repositories\n* Integration of various external code repositories\n\n===== Orchestration\n\nTo *orchestrate* pipelines the two following basic mechanisms can be used:\n\n* Implicit Chaining\n+\nIn that case the complete workflow is not explicitly coded in a dedicated pipeline. Pipelines are chained implicitly by triggering events. The biggest problem with that approach is the missing single pane of control. The current state in the overall workflow is for instance only implicitly given by the currently running pipeline.\n\n* Creating a dedicated orchestration pipeline\n+\nAn additional pipeline triggers in this scenario other pipelines acting as building blocks. Pipelines can run separately (just run the deployment) or as part of a bigger workflow (e.g. create environment from scratch).\n\nOrchestration must take dependencies into account. They might result from the deployed code or the scope of the pipeline (scope is e.g. a single microservice and code includes the libraries neede).\nOrchestrated pipelines must pass data between them. The recommended method is to use key vault.\n\n*Recreation of resources in short intervals* can cause pipelines to fail since the previously deleted resource still exists in the background.(Even although soft delete is not applicable). Whether Azure really deleted everything depends on the service. For instance Azure API management seemed to be affected by that problem.\n\n===== Traceability\n\n*Traceability* requires an identifier for referencing artefacts. A standard schema is a semantic version. The platform only supports partial support for number generation such as https://ychetankumarsarma.medium.com/build-versioning-in-azure-devops-pipelines-94b5a79f80a0[incrementing numbers]. Linking the code in the repo to a certain version depends on used repository. \n\n==== Infrastructure/ Application code\n\nA *programming language* is either \"declarative\" or \"imperative\". Declarative programming languages state the target state and it is the job of the declarative programming language how to get there. The following rules are applied to achieve that:\n\n* create a resource if not there\n* update an existing resource if different properties\n* delete resource if not there\n\nImperative programming languages state the how. The internal delta calculation needs to be explicitly programmed here. If possible declarative programming languages are recommended due to automatic delta calculation. Typical case is infrastructure.\n\nTypical declarative options are shown in detail in the table below. The overall recommendation is to go for terraform. Major reasons for downvoting Bicep/ ARM:\n\n* ARM: difficult readability for humans\n* Bicep: Lack of support for testing based on plan and testing ecosystem since first added recently.\n\nTable with declarative programming language options:\n[options=\"header\"]\n|=======================\n|Criteria|Bicep      |ARM | Terraform\n|Same syntax across clouds |- (Azure Only)     |- (Azure Only)   |+ (multi)\n|What if    |o (no complete prop list;only display of plan; unexpected delete)     |- (not available)   |+ (plan command)\n|Detection current    |o (Real anaylsis but time)     |+ (Real anaylsis)   |o (Statefile)\n|Testing/ static analysis    |o (Only via ARM)|+ (available)   |+ (available)\n|Human Readability    |+ |- |+\n|Reverse Engineering    |- (Extra ARM step + adjust) |o (adjust) |+ (Direct via Terraformer)\n|Latest features    |o (No embedded fallback) |+ (native) |o (Time lag but embedded fallback)\n|=======================\n\nThe major options for imperative programming languages are Azure CLI, Powershell (Windows) or Linux based scripting. Azure CLI is recommended as prefered choice since it works on linux and windows based VMs.\n\nThe created resources should follow a *uniform naming schema*. This requires naming to be factored out in a centralized module. Terraform supports factoring out common code in modules. However the backend must already exist and should also follow a naming convention. The recommendation is therefore to expose the common terraform module via an additional path that does not require a backend to determine the names for the azure resources representing the backend. \n\n==== Provisioning\n===== Organizational Mapping\n\nThe provisioning must match the organizational requirements of your organization. Azure provides services to model sub units within your organization such as departments, projects and teams.\n\n===== Integration\n\nPlatform allows a modular approach to outsource certain functionality to third party software such as code repository. Which parts is service specific.\n\nExternal tools providing pipelines can be integrated in two conceptual ways:\n\n* *Trigger automation pipelines from external:* This involves the configuration of a CI pipeline in the external tool such as Jenkins and mechanism in the automation service that invokes the CI process when source code is pushed to a repository or a branch.\n* *Run external pipelines from within the platform:* In this approach automation reaches out to an external tool to work with the results.\n\n===== Configuration\n\nConfiguration for provisioning is required in various areas:\n\n* *Environment:* E.g. name of resource group per potential target environment\n* *Repository:* E.g. relevant repos/ branching\n* *Pipelines:* Parameters pipelines run with such as the technical user name or settings required by the built/ deployed code.\n\nConcrete features used for the above three points depend on the used services. A general storage for sensitive data (keys, secrets, certificates) in Azure is always Azure Key Vault.\n\n===== Compliance\n\nThe standard concept for role-based access controls is called RBAC in Azure. It assigns principals (humans or technical accounts) permissions for a certain resource. Regarding provisioning the following users are relevant:\n\n* Technical user (service principal) the pipelines are running with\n* Users for administrating the provisioning service\n\nAzure Active Directory is the central service in Azure that defines and controls all principals (human/ service) per tenant.\n\nGranularity of roles that can be granted depend on the service. The boundaries in which users exist or permissions can be assigned is also service specific.\n\n\n== Credits\n\nimage::ms_guild_logo.png[MS Guild Logo, width=160, height=75, align=right, link=\"https://forms.office.com/Pages/ResponsePage.aspx?id=Wq6idgCfa0-V7V0z13xNYal7m2EdcFdNsyBBMUiro4NUNllHQTlPNU9QV1JRRjk3TTAwVUJCNThTRSQlQCN0PWcu\"]\n"}