[{"dirname":"architecture_hexgagonal","id":0,"path":"target/generated-docs/solutions/architecture_hexgagonal/index.html","type":"solution","title":"Hexagonal architecture","body":"\nHexagonal architecture\nThis article explains so-called Hexagonal Architecture which is one of architectural styles applicable for wide variety of applications / software.\nYou have the following problem to be solved\nWe need to develop our software in more robust way so that it can survive technological changes.\nYou should be not forced to rewrite big amount of business logic code just because you need to:\nreplace specific transport layer protocol,\nor replace storage technology,\nor you want to migrate from one cloud provider to another cloud provider,\nor …​\nEventually, you don’t want your domain code base to be scattered and obscured with technological details such as HTTP exceptions, SQL code, JPA/Spring annotations, etc.\nThe proposed solution enables the customer\nWith appropriate architecture that is technology agnostic the customer should expect:\npredictable and limited costs of migration to newer version of technological components (including \"breaking changes\" migrations),\nmanageable costs of replacing technological components (such as exchanging SQL provider or migrating from Springboot to Quarkus),\nmanageable costs of infrastructure shifts (such as changing cloud provider),\nlower overall maintenance costs due to cleaner domain code and higher software quality.\nThe proposed solution is\naddressing your business\nThis solution is most valuable for medium- to high-complexity software systems.\nExpected benefits are measured as further development and maintenance costs as presented on the following pseudo-graph by Martin Fowler (see bibliography for source).\nHexagonal architecture can be applied to any business domain, however it works best with domains that have rich business rules.\ndefinitively needed if\nHexagonal architecture is preferred solution if:\nthere is a high probability of exchanging certain technical component in the future,\nthere is a necessity of applying a DDD technique (Domain Driven Design).\nHow to solve your problem\nBe domain centric\nPlan and implement your domain model without any technical dependencies.\nUse standard features of your programming language as they are sufficient to realize your goals.\nYou can use power of object oriented programming and model entities or aggregates with methods mutating their state.\nBe use case centric\nPlan and implement your use cases without any technical dependencies.\nUse domain model as the only dependency to your use cases.\nThe use case is basically a function that uses and interacts with domain model.\nThe domain and use cases form \"the core\" as shown in the center of following diagram.\nIsolate and protect your core\nKeep your domain model and use cases isolated from technical details.\nAs each use case is basically a function, call this function from glue code called inbound adapter.\nThe interface between your use case and the adapter is called inbound port.\nConvert data from inbound specific form into use case specific form if necessary.\nIf your use case must call any external system (that is usually another API, event queue, database), use inversion of control pattern and hide that external system behind interface.\nWe call such interface an outbound port.\nManage your dependencies\nYour ultimate goal is to keep your system core clean of any technical dependencies.\nIt makes more sense to present such architecture using concentric circles rather than stacked layers.\nWith such visualisation in mind you have to ensure that dependencies always point inwards and never in opposite direction (refer dashed arrows on following diagram).\nIt is important to note, that layers inside the core, as visible in \"onion\" model are optional.\nYou can put as many layers as you want, depending on needs.\nThe bare minimum seems to be however: domain and use cases.\nUse appropriate tooling\nTo practically manage dependencies you can use:\nmultiple source modules (using Maven or Gradle),\nArchUnit,\ncombine both if needed.\nGo beyond!\nRelated documentation\nThe Hexagonal Architecture by Alistair Cockburn\nThe Clean Architecture by Robert C. Martin\nThe Onion Architecture by Jeffrey Palermon\nClean Architecture: A Craftsman’s Guide to Software Structure and Design, Robert C. Martin\nGet your hands dirty on clean architecture, Tom Hombergs\nIs quality worth costs by Martin Fowler\nPractical example by Herberto Graca\nHex Thai Star - reimplementation of My-Thai-Star (WiP)\nReady for changes with Hexagonal Architecture by Damir Svrtan and Sergii Makagon\n"},{"dirname":"cloud_storage_overview","id":1,"path":"target/generated-docs/solutions/cloud_storage_overview/index.html","type":"solution","title":"Cloud Storage","body":"\nCloud Storage\nThis article mainly focuses on below topics:\n- What is Cloud Storage?\n- How does it works?\n- Benefits of Cloud Storage\n- Cloud Storage requirement\n- Types of Cloud Storage\n- Product and services offered by different vendors\nWhat is Cloud Storage?\nCloud Storage is a cloud computing model in which data is transmitted and stored on remote storage systems, where it is maintained, managed, backed up and made available to users over a network — typically, the internet.\nIt’s delivered on demand with just-in-time capacity and costs, and eliminates buying and managing your own data storage infrastructure. This gives you agility, global scale and durability, with “anytime, anywhere” data access.\nHow does it works?\nCloud Storage is purchased from a third party cloud vendor who owns and operates data storage capacity and delivers it over the Internet in a pay-as-you-go model. Typically, you connect to the storage cloud either through the internet or a dedicated private connection, using a web portal, website, or a mobile app. The server with which you connect forwards your data to a pool of servers located in one or more data centers, depending on the size of the cloud provider’s operation.Applications access Cloud Storage through traditional storage protocols or directly via an API.\nThese Cloud Storage vendors manage capacity, security and durability to make data accessible to your applications all around the world.\nCloud Storage is available in private, public and hybrid clouds.\nPublic Cloud Storage: In this model, you connect over the internet to a storage cloud that’s maintained by a cloud provider and used by other companies.\nPrivate Cloud Storage: Private Cloud Storage setups typically replicate the cloud model, but they reside within your network, leveraging a physical server to create instances of virtual servers to increase capacity. You can choose to take full control of an on-premise private cloud or engage a Cloud Storage provider to build a dedicated private cloud that you can access with a private connection.\nHybrid Cloud Storage: This model combines elements of private and public clouds, giving organizations a choice of which data to store in which cloud.\nBenefits of Cloud Storage\nTotal Cost of Ownership:\nWith Cloud Storage, there is no hardware to purchase, storage to provision, or capital being used for \"someday\" scenarios. You can add or remove capacity on demand, quickly change performance and retention characteristics, and only pay for storage that you actually use.\nLess frequently accessed data can even be automatically moved to lower cost tiers in accordance with auditable rules, driving economies of scale.\nTime to Deployment:\nCloud Storage allows IT to quickly deliver the exact amount of storage needed, right when it’s needed. This allows IT to focus on solving complex application problems instead of having to manage storage systems.\nInformation Management:\nCentralizing storage in the cloud creates a tremendous leverage point for new use cases. By using Cloud Storage lifecycle management policies, you can perform powerful information management tasks including automated tiering or locking down data in support of compliance requirements.\nScalability: Growth constraints are one of the most severe limitations of on-premise storage. With Cloud Storage, you can scale up as much as you need. Capacity is virtually unlimited.\nLimitations of Cloud Storage\nSecurity: Security concerns are common with cloud-based services. Cloud Storage providers try to secure their infrastructure with up-to-date technologies and practices,but occasional breaches have occurred, creating discomfort with users. Security is shared responsibility with cloud providers and users.\nLatency: Delays in data transmission to and from the cloud can occur as a result of traffic congestion, especially when you use shared public internet connections.\nRegulatory compilance: Certain industries, such as healthcare and finance, have to comply with strict data privacy and archival regulations, which may prevent companies from using Cloud Storage for certain types of files, such as medical and investment records.\nType of Cloud Storage\nMajorly, below are main types of Cloud Storage:\nObject Storage: It manages data as objects. Each object includes the data in a file, its associated metadata, and an identifier. Objects store data in the format it arrives in and makes it possible to customize metadata in ways that make the data easier to access and analyze. Instead of being organized in files or folder hierarchies, objects are kept in repositories that deliver virtually unlimited scalability. Since there is no filing hierarchy and the metadata is customizable, object storage allows you to optimize storage resources in a cost-effective way.\nFor example, S3(Simple Storage Service) in AWS, Blob Storage in Azure and Google Cloud Storage in GCP are object storage.\nFile Storage: The file storage method saves data in the hierarchical file and folder structure with which most of us are familiar. The data retains its format, whether residing in the storage system or in the client where it originates, and the hierarchy makes it easier and more intuitive to find and retrieve files when needed. File storage is commonly used for development platforms, home directories, and repositories for video, audio, and other files.\nFor example, EFS and FSx are file storage services in AWS, azure file storage in Azure and Google Cloud Filestore in GCP.\nBlock Storage: Block storage, sometimes referred to as block-level storage, is a technology that is used to store data files on Storage Area Networks (SANs) or cloud-based storage environments. Developers favor block storage for computing situations where they require fast, efficient, and reliable data transportation.Block storage breaks up data into blocks and then stores those blocks as separate pieces, each with a unique identifier. The SAN places those blocks of data wherever it is most efficient.\nBlock storage also decouples data from user environments, allowing that data to be spread across multiple environments. This creates multiple paths to the data and allows the user to retrieve it quickly.\nFor example, EBS in AWS and Google Cloud Persistent Disks in GCP.\nAlso, archival and database services can be considered for data storage\nProducts &amp; Services\nBelow table contains services from different cloud vendors:\nVendor\nStorage Services\nDatabase Services\nBackup Services\nAWS\n• Simple Storage Service (S3)\n• Elastic Block Storage (EBS)\n• Elastic File System (EFS)\n• Storage Gateway\n• Snowball\n• Snowball Edge\n• Snowmobile\n• Aurora\n• RDS\n• DynamoDB\n• ElastiCache\n• Redshift\n• Neptune\nGlacier\nAzure\n• Blob Storage\n• Queue Storage\n• File Storage\n• Disk Storage\n• Data Lake Store\n• SQL Database\n• Database for MySQL\n• Database for PostgreSQL\n• Data Warehouse\n• Server Stretch Database\n• Cosmos DB\n• Table Storage\n• Redis Cache\n• Data Factory\n• Archive Storage\n• Backup\n• Site Recovery\nGCP\n• Cloud Storage\n• Persistent Disk\n• Transfer Appliance\n• Transfer Service\n• Cloud SQL\n• Cloud Bigtable\n• Cloud Spanner\n• Cloud Datastore\nNone\nReferences:\nhttps://www.ibm.com/cloud/learn/cloud-storage#toc-what-is-cl-vt64lltQ\nhttps://aws.amazon.com/what-is-cloud-storage/\n"},{"dirname":"communication","id":2,"path":"target/generated-docs/solutions/communication/index.html","type":"solution","title":"Communication among services","body":"\nCommunication among services\nCommunication is a very important aspect while building IT application, especially large and complex one or one with microservices architecture. Based on your analysis of your problems, considering the following basic questions to start with:\nWhich services have to communicate with which services? to exchange which type of data?\nHow should the communication look like? in a synchronous way or asynchronous way?\nOnce you have your answers, make your choices on concrete protocols, products and libraries. Using multiple of them in your application is fine.\nUnfortunately, there is no silver bullet. Each communication type has its own advantages and disadvantages and target a different scenario and goals. With that said, do not always apply REST or always apply message-driven approaches (e.g. Kafka) just because you have heard some success stories about it.\nHere we try to expose cross-cutting best practices for communication among services in larger IT application landscapes on a high level. Details how to implement this with specific libraries or programming-languages are described in the individual stacks of devonfw.\nCommunication types\nGenerally, types of communication can be classified in two axes.\nThe first axis defines if the protocol is synchronous or asynchronous:\nSynchronous protocol: The client sends a request and waits for a response from the service as the client code can only continue its task when it receives the server response. That’s independent of the client code execution that could be synchronous (thread is blocked) or asynchronous (thread isn’t blocked, and the response will reach a callback eventually). e.g. HTTP/HTTPs\nAsynchronous protocol: The client code or message sender usually doesn’t wait for a response. It just sends the message as when sending a message to a queue or any other message broker. e.g AMQP\nThe second axis defines if the communication has a single receiver or multiple receivers:\nSingle receiver: Each request must be processed by exactly one receiver or service.\nMultiple receivers: Each request can be processed by zero to multiple receivers. This type of communication must be asynchronous. An example is the publish/subscribe mechanism.\nOne common style is single-receiver communication with a synchronous protocol like HTTP/HTTPS when invoking a regular Web API HTTP service. Microservices also typically use messaging protocols for asynchronous communication between microservices. We will discuss more about some popular approaches adopted these days including:\n[rpc]\n[messaging_and_eventing]\n[service_mesh]\nRPC\nWhen it comes to RPC communication nowadays we immediately talk about HTTPS.\nThe most common choice is REST with JSON that perfectly works together with web browsers.\nHowever, please also consider other options especially for backend communication:\ngRPC is an efficient, high performance RPC protocol\n…​\nProtocols like SOAP or even RMI, Corba, etc. should be considered as legacy and discouraged.\nMessaging and eventing\nWe do not even try to distinguish between messaging and eventing.\nA message can be seen as an event and an event as a message.\nThe nature of this communication style is that it is entirely asynchronous and the sender of the event or message does not have to know about the receipient(s).\nThere can even be many recipients for the same event or message.\nTypically there is a central messaging system (event bus) that is all you need to know about to send and receive your events or messages.\nThis leads to a very loose coupling of your services.\nWhile this adds flexibility it also can add quite some complexity to understand what is actually going on.\nDebugging and tracing the communication can get really hard.\nIn the worst case you can lose control and end up with cyclic events triggering each other till eternity.\nHowever, when properly applied, this communication style can make your architecture very powerful and extendable.\nOne of the most common choices for such a messaging system is kafka. Other message brokers to be considered are RabbitMQ and ActiveMQ\nService mesh\nIn the context of microservices, service mesh is a key to communication.\nService-to-service communication is what makes microservices possible.\nBut creating, routing and managing this communication, both within and across application clusters, becomes increasingly complex as the number of services grows.\nService mesh solves this problem. It takes the logic governing service-to-service communication out of individual services and abstracts it to a layer of infrastructure.\nIt also captures every aspect of the communication as performance metrics which can be used to monitor the cluster as well as identify and analyze system failures if occurs.\nOne of the most common and best choices for service mesh is istio.\nIstio is an open-source service mesh with powerful features that enable developers to easily configure\nthe traffic flow\nthe authentication and authorization among internal and to external services and\nmonitoring tools such as kiali, prometheus and grafana.\nCommunication contracts\nA communication contract is a structured document that defines the expected input and output of a service.\nThere are two approaches to using contracts for communications:\nContract first: There is a design document that defines the formal contract of the communication and the code artefacts are derived with some tooling.\nCode first: Code is written directly and documentation is generated from there.\nFor enterprise development, contract first approach is preferred because it improves communication among involved development teams. These contracts can serve to develop tests, mocks and working code automatically and to enforce validation on both client and server sides of the services.\nContracts can be used both for synchronous or asynchronous communications, for example Open Api has become the standard for REST contracts and Async Api is growing in adoption for the asynchronous needs.\nLinks\nREST vs messaging for microservices\nEventing and messaging\nKafka microservices\nCommunication in a microservices architecture\nGetting started with istio\nKeycloak and istio\nImplementation hints\nREST:\nJava Server\nJava Client\nAngular\n.NET/C#\nnode.js\nKafka:\nJava\n"},{"dirname":"microsvc_platforms_intro","id":3,"path":"target/generated-docs/solutions/microsvc_platforms_intro/index.html","type":"solution","title":"Microservice Platforms Introduction","body":"\nMicroservice Platforms Introduction\nContext &amp; Problem\nMicroservice orchestration is the automatic process of managing or scheduling the work of individual microservices of an application within multiple clusters. The platform provides an automated process of managing, scaling, and maintaining microservices of an applications.\nThe container orchestration platform kubernetes is the de facto standard in the meantime (Docker swarm and others are ngelectable compared to Kubernetes in the meantime). Containers are executable units of software containing application code, libraries, and dependencies so that the application can be run anywhere.\nContainer orchestration tools automate the management of several tasks that software teams encounter in a container’s lifecycle, including the following: Deployment, Scaling and load balancing/traffic routing, Networking, Insights, Provisioning, Configuration and scheduling, Allocation of resources, Moving to physical hosts, Service discovery, Health monitoring, Cluster management (Link).\nThe picture below summarizes the major aspects:\nStandard Problems\nThe following standard problems will be addressed in subsequent paragraphs:\nOrchestration Platform:\nTo be detailed:\nDeplyoment (of resources into cluster)\nScaling\nload balancing/traffic routing\nNetworking\nConfiguration and scheduling\nService discovery\nApplication services like DAPR\nResources to be deployed:\nTo be detailed:\n* Resources (Containers but also other Kubernetes objects)\n* Building of containers\n* Registry\nContainer Platform:\nAffect containers running on/ platform\nTo be detailed:\n* monitoring\n* Security\n* Provisioning\nFor the following aspects check out the other defined patterns:\nMonitoring infrastructure and application code\nGeneral guidelines for structuring repositories (e.g. mono vs. multi-repo)\nGeneral guidelines for defining landing zones\nAzure as Platform\nAzure provides container platforms that address major concerns:\nCluster orchestration ⇒ What Kubernetes\nService Meshes ⇒ what they cover\nApplication patterns ⇒ DAPR provides application patterns like publish/ subscribe and abstracts from Kubernetes\nAzure Arc ⇒ extends control plane to non Azure hosted Kubernetes clusters (e.g. on-premise, other clouds)\n"},{"dirname":"microsvc_platforms_intro_azure_aks","id":4,"path":"target/generated-docs/solutions/microsvc_platforms_intro_azure_aks/index.html","type":"solution","title":"Azure Microservice Platforms Solution AKS","body":"\nAzure Microservice Platforms Solution AKS\nOverview\nTODO kubernetes core\nThe services that (can) complement Kubernetes:\nProvisioning\nSee Provisioning for general aspects how you can handle provisioing in Azure such as creating pipelines for creating infrastructure. Additional aspects such as building a containe image are described here.\nAzure Container Registry\nAzure AD\nAzure Active Directory provides the service principal the pipelines run with.\nMonitoring\nFor infrastructure monitoring see \"Monitoring\". For application monitoring specific tools exist which will be described here.\nThe picture illustrates the setup with the major dependencies:\nPattern Details\nGeting Started\nTODO if there are options what do I have to decide first e.g. organizations, projects and teams in case of Azure DevOps\nRemaining goals\nPoints from summary Migration:\nDeployment resources to Kubernetes\nInstead of having to write separate YAML files for each application manually, you can simply create a Helm chart and let Helm deploy the application to the cluster for you. Helm charts contain templates for various Kubernetes resources that combine to form an application. A Helm chart can be customized when deploying it on different Kubernetes clusters. Helm charts can be created in such a way that environment or deployment-specific configurations can be extracted out to a separate file so that these values can be specified when the Helm chart is deployed. For example, you need not have separate charts for deploying an application in development, staging, and production environments.\nChart linting is an easy tool that you can add to your pipeline to ensure your deployments are valid and versioned correctly.\nFor security reasons and improvement of Helm charts, it is useful to make use of at least one Helm linting tool.\nWhy choosing Polaris as Linting Tool:\nFor helm chart linting, there are several tools like Polaris, kube-score or config-lint available. With Polaris, checks and rules are already given by default, whereby other tools need a lot of custom rules configuration and are therefore more complex to setup.\nPolaris runs a variety of checks to ensure that Kubernetes pods and controllers are configured using best practices, helping to avoid problems in the future.\nPolaris can be either installed inside a cluster or as a command-line tool to analyze Kubernetes manifests statically.\nThe Helm operator provides an extension to Flux that automates Helm Chart releases. A Helm Chart release is described via a Kubernetes custom resource named HelmRelease. Flux synchronizes these resources from Git to the cluster, while the Helm operator makes sure Helm Charts are released as specified in the resources.\nConfiguration\nAzure Key Vault to Kubernetes (akv2k8s) is used for our applications, to make Azure Key Vault secrets, certificates and keys available to use in a secure way.\nThe goals of Azure Key Vault to Kubernetes are to avoid a direct program dependency for getting secrets, secure and low risk to transfer Azure Key Vault secrets and transparently inject secrets into applications.\nPer default secrets, configurations and certificates can be easily read and accessed by users in Kubernetes and access to them can only be restricted by setting access rights. This will be avoided and is a huge benefit of using akv2k8s with a simple setup of Azure Key Vault and the option to set more detailed restrictions and configurations.\nQuestion: What about telling container what is the right key vault? (Injection needs to know about environment)\nTools for synchronizing configuration:\nArgoCD:\nContinuous Delivery for Kubernetes: ArgoCD is a leader developed by Intuit and synchronizes changes in the code of applications, photos and cluster definitions, so the Git - repository , to the cluster. The solution is open source software, kept relatively simple and is one of the most important and oldest tools on the market.\nFlux\nIntegration of Flux CD into the workflow: Flux basically does the same job as ArgoCD, i.e. synchronizes the repository and cluster in the course of continuous delivery. Flux is also open source and kept simple - the special thing: It comes from the GitOps inventor Weaveworks.\nQuestion: Have heard about built in Flux support in AKS\nMonitoring\nApplication monitoring: Prometheus\nInfrastructure: see Standard (Extend dashboarding)\nVariations\nThe following alternatives exist:\nManaged Openshift\nComment Openshift: Refers to Red Hat Openshift and not hosted offering\nTODO: Better variation for me managed OpenShift (Link)\nCriteria\nOpenshift\nKubernetes\nFlexibility\nLimited; opinionated components/ fnctionalities\ngreater flexibility\nInstallation\nlimited options\nalmost anywhere\nSecurity\nvery strict; certain permission level for maintenance\nDeployment\nless flexible DeplyomentConfig\nvery flexile helm charts\nExternal access\nvia Routers\nvia ingress conntrollers\nManagement\nImageStreams improve management\nManagement container images not that easy\nUsere experience\nBetter user support\nadditional tool for user experience\nNetworking\nnative networking solution\nsome components don’t have networking + third party required\nService Catalog\nWhen to use\nWhen you need orchestration support due to a higher number of microservices. If you start with a single service then Function App reduces greatly the complexity. You can still later on move to Kubernetes by containerizing your function app code.\n"},{"dirname":"monitoring_cloudwatchCustomAlarm","id":5,"path":"target/generated-docs/solutions/monitoring_cloudwatchCustomAlarm/index.html","type":"solution","title":"Customise your cloudwatch alerts","body":"\nTable of Contents\nCustomise your cloudwatch alerts\nYou have the following problem to be solved\nThe proposed solution enables the customer\nRelated Architectures and Alternatives\nProducts &amp; Services\nCustomise your cloudwatch alerts\nYou have the following problem to be solved\nCloudwatch alarms give you the ability to notify you in case an alarm is triggered. The standard message layout of cloudwatch is very inflexible and not customisable. With this approach it is possible to fill HTML templates with further information, links and buttons to find the solution ASAP.\nThe proposed solution enables the customer\nto send out an customised e-mail template which looks more professional and has additional information compared to the standard cloudwatch alarm.\nRelated Architectures and Alternatives\nSee code here: https://github.com/AlessandroVol23/cloudwatch-custom-email-cdk\nProducts &amp; Services\nCloudwatch Alarm: Some cloudwatch alarm\nSNS Topic which will be triggered by the cloudwatch alarm\nLambda which will be called by the SNS topic\nSES will be used by the lambda to send out HTML emails\nAbout\nAbout devonfw\nFeatures\nTechnology Stack\nExplore\nGetting started\nArchitecture\nResources\nDocs\nUser guide\nReleases information\nWiki\nCommunity\nContributors\nWebsite Contribution\nTerms of Use\nSupport\nimport { UtilsModule } from '/website/shared/utils.js';\nimport { HeaderModule } from '/website/components/header/header.js';\nlet searchData = { index: null, documents: null };\nUtilsModule.loadIndex(searchData);\n$(window).on('load', function() {\nconst queryFun = () => {\nHeaderModule.queryFunction(searchData);\n};\nHeaderModule.searchOnClick(queryFun);\n});\nlet bb = document.getElementById('menu-button');\nbb.addEventListener('click', function() {\ndocument.querySelector('.website-navbar ul').classList.toggle('visible');\nconsole.log(document.querySelector('.website-navbar ul'))\n})\nimport { EditLinksModule } from '/website/shared/editlinks.js';\nlet alwaysVisible = true;\nif(document.location.pathname.endsWith(\"pages/welcome/welcome.html\")) {\nalwaysVisible = false;\n}\nEditLinksModule.addEditLinks(alwaysVisible);\nLast updated 2021-09-08 10:06:18 UTC\n"},{"dirname":"monitoring_in_azure","id":6,"path":"target/generated-docs/solutions/monitoring_in_azure/index.html","type":"solution","title":"Azure Monitoring","body":"\nAzure Monitoring\nContext &amp; Problem\nTerminology\nUsed definition: A monitoring solution helps the monitoring consumer achieve the satisfactory level of control of a defined service. [1]\nThis definition already includes the following:\nDefined service: The resources you want to monitor aka monitored resources.\nLevel of control: That is your bandwidth in which your defined service operates normally aka known as baseline\nMeasuring: A measurement is a single act that quantifies an attribute of a part, equipment, service or process (CPU load, available memory etc.). Data measured is emitted by the monitored resources and aka telemetry.\nMonitoring consumer: The user trying to keep the service within its baseline boundaries. A single control plane is usually preferred to simplify the operations for the consumer aka monitoring plane. Depending on its perspective the area of cous might differ such as performance, costs, security etc. Independent from the underlying platform.\nActions might be triggered by the system or the consumer to achieve that. This might include:\nVisualize current state\nDetect Deviations from baseline\nRoot cause analysis\nForward externally\nMonitoring should be implemented as feedback loop where lessons learnt are the starting point for further improvements on the defined service side. E.g. by adaptive scaling depending on monitored traffic.\nThe use case defines what is the defined service and the perspectives and control level to be achieved. It also defines external dependencies/ preconditions that have to be considered. Deployment of monitored resources might also be part of it. The platform provides the technical capabilities on which the sue case is implemented. The picture below illustrates this:\nStandard Problems\nA general solution for all potential problems is not possible due to the multitude of different requirements. The idea is therefore to address certain standard features which are as follows:\nVisualizations\nMeasuring\nImproving Feedback Loop\nOptimization of alerts\nCorrelation of telemetry entries\nArchiving Telemetry Data\nAzure as Platform\nThe possibilities depend heavily on the perspective and the resources you want to monitor. A major tool to implement the monitoring plane is Azure Monitor with its features Application Insights and Log Analytics. The subsequent chapters focus on these major services. However, other services depending on your perspective might be relevant too. The following services relevant by category:\nPerformance: Azure Monitor (Network Monitor)\nAvailability: Azure Service Health, Azure Resource Health\nSecurity: Azure Service Health, Azure Sentinel, Azure AD\nCosts: Cost Management\nThe following conventions apply regarding the measurement for monitored resources:\nPush to monitoring plane\nIn that case telemetry is forwarded to the monitoring plane. This can be necessary if the telemetry is not available in Azure monitor out of the box or pulling from the monitored resources is not possible. Examples of forwarding that requires explicit activation:\nDiagnostic setting (differs per resource): requires a diagnostic setting to be activated.\nApp Insights Instrumentation/ Linking: Linked App Insights must be specified for the monitored resource\nManual forwarding: e.g. by scheduled process using API provided by Azure Monitor if the exporting options are not granular enough\nThis approach requires additional data sources on in the monitoring plane which might be inside Azure monitor (Log Analytics Workspace/ App Insights) or external.\nPull from monitored resource\nIn that case the telemetry data is read directly from the monitoring plane such as metrics. Logs cannot be read directly and require pushing. Compared to pushing this method is also faster.\nAzure Monitor as monitoring plane covers the following major features:\nTelemetry Analytics\nKusto is addressing that need and allows you to access external data sources such as blobs and internal data sources inside Azure Monitor. Graphical representation is also possible.\nVisualization (Workbooks, Dashboards, other services)\nAlerts\nTo react automatically if outside operational boundaries. Results from Kusto queries can be used as trigger.\nRoot Cause analysis (TODO)\nApplication Map ⇒ application dependencies in other services such as backend APIs or databases\nSmart Detection ⇒ warn you when anomalies in performance or utilization patterns\nUsage Analysis ⇒ features of your application are most frequently used\nRelease annotations ⇒ visual indicators in your Application Insights charts of new builds and other events. Possible to correlate changes in application performance to code releases.\nCross-component transaction diagnostics ⇒ shows you broken piece in the entire transaction\nSnapshot Debugger ⇒ collect a snapshot of a live application in case of an exception, to analyze it at a later stage.\nIntegration\nAzure Monitor has also extensive integration features. This includes:\nIntegrating telemetry from other Azure services (e.g. Azure Security Center also forwards to Azure Monitor)\nIntegrating external data sources (e.g. Blobs by using Kusto external operator)\nIntegrating third party tools such as Prometheus for Azure Kuberenetes\nExposing for data sources for external third party (e.g. Log Analytics Workspaces for Grafana)\nAzure Monitor pricing comes with the following:\nIngestion: Applies for additional data pushed to Azure monitor\nStorage: Data stored within Azure Monitor costs ⇒ Long term Archiving solution must be therefore found\nAlerts: Are charged as well ⇒ strategy for minimizing them is required\nSolution\nOverview\nThe solution is to use Azure Monitor and its features. The subsequent detail variations that can be used for solving the problems outlined above.\nVariations\nVisualization\nVisualization requires the following points:\nProviding a canvas\nCanvas refers to the area on which you place carious components. The following options exist:\nAzure\nThird party\nWorkbooks\nDashboards\nPower BI\nGrafana\nAuto refresh in 5 Min Intervall\nX\nX\n???\nFull screen\nX\n???\n???\nTabs\nX\n???\n???\nFixed Parameter lists\nX\n???\nX\nDrill down\nX\nX\nAdditional hosting required\nX\nTerraform Support\nX\nX\nX\nRegarding components for logs/ metrics:\nMetrics: Pull (Metrics explorer) or push (Kusto query targeting data source) possible\nLogs: Push to monitoring plane only\nGrafana can be used for visualization via using a connector for log analytics workspace\nData source\nCan be inside Azure Monitor or external. External stores can avoid high Azure Monitor costs for ingestion/ storage.\nNOTE Referencing an external data source requires authentication e.g. by using a shared access signature for a blob. Updating a saved query is only possible for log analytics.\nMeasuring\nThe table below shows possible options:\nDiagnostic Settings\nApp Insights\nPush via resource API\nMetrics Explorer\nPossible per resource\n(X)\n(X)\nX\n(X)\nTelemetry Customization\nLimited\nHigh\nLimited-High\nLimited\nCustom Logging in executed code\nX\nTelemetry always captured\nX\n(X)\nX\nX\nLatency\nMedium\nMedium\nMedium\nLow\nDirection\nPush\nPush\nPush\nPull\nComments:\nOption “Push via resource API” ⇒ A scheduled script that reads periodically telemetry and pushes it to monitoring plane using the Rest API\n„Telemetry always captured“ ⇒ Some resources allow multiple ways to run something e.g. via UI or programmatically. If the telemetry is always captured the way does not matter.\nArchiving\nA good archiving store is blob storage. Lifecycle policies can be used to drop the blob after a predefined amount of time.\nWhen to use\nThis solution assumes that your control plane is in Azure and that your monitored resources are located in Azure.\n"},{"dirname":"monitoring_openTelemetry","id":7,"path":"target/generated-docs/solutions/monitoring_openTelemetry/index.html","type":"solution","title":"Monitoring your microservices with openTelemetry","body":"\nMonitoring your microservices with openTelemetry\nWhere is the problem?\nWith the complexity and size of microservice systems, which include hundreds of small services, keeping an overview can be quite challenging. Usually one monolitic application can be scanned for potential errors, as failures in the system directly reference this one application. With microservices the area to search in could be narrowed down, but never to an extend adressing one specific service being the root cause. Therefore, to keep track of runtime problems and the fullfillment of nonfunctional reqirements corresponding to specififc microservices advanced monitoring needs emerge.\nThere are different proposals for collecting this telemetry data consisting of logs, traces and metrics with each proposal having their own benefits and disadvantages leading to a heterogenous landscape of these solutions special for each microservice. As these are configured on their own and little to no replacability options without nudging big change efforts are given, flexibility is limited blocking possibly better solutions. Concluding, overall and in the microservice teams there is not a central repository for looking up your microservices telemetry data but again a numerous amount of different hardly replacable solutions for different microservices making it difficult to gain a central overview e.g. via Grafana.\nThe value for the customer\nMonitoring in microservice settings generally benefits nonfunctional requirements such as performance, availability or security through transparent insights, while enabling proactive but also reactive handling of issues. By having all the telemetry data centrally stored and providing exchangeable solutions for the different types of data, no restrictions to design decisions is made and the customer can gain a fast overview over a great amount of microservices.\nIntroducing OpenTelemetry\nOpenTelemetry forms the combination of OpenTracing and OpenCensus collecting different telemetry data (metrics, logs, traces) for understanding the applications behavior and performance. Because of the standardization for processing telemetry data, OpenTelemetry acts as a central collector for whole application landscapes monitoring data, which is able to communicate with replacable logging-, tracing- or metrics-backends without changing configurations in the applications code.\nAddressing your business\nOpenTelemetry can be used in various use cases, but is best suited in the microservice context. Therefore, any business apllication relying on microservices would be a suited domain for applying an OpenTelemetry solution.\nOpenTelemetry architecture\nThe above example shows a reference architecture of multiple hosts (environments/applications).\nTraces and logs are automatically collected at each JAX-RS service. Other additionally defined metrics, traces or logs are also collected.\nThese applications send data directly to a otel-agent configured to use fewer resources. An otel-agent is a collector instance running with the application or on the same host as the application.\nThe agent then forwards the data to a collector that receives data from multiple agents. Collectors on this layer typically are allowed to use more resources and queue more data.\nThe collector then sends the data to the appropriate backend, in the solution provided by Jaeger, Zipkin or VictoriaMetrics. The Victoria backend serves metrics while Jaeger and Zipkin are two alternatives for tracing._\nAdditionally all the telemetry data can be visualized in a tool like Grafana.\nThe OpenTelemetry collector is an instance that makes it possible to receive telemetry data, optionally transform it and send the data on. Receiving, transforming and sending data is done via pipelines. A pipeline consists of a set of receivers, a set of optional processors and a set of exporters.\nA reveiver transfers the telemetry data into the collector, which then can be processed on a processor. Finally, an exporter can send the data to a corresponding backend/destination. Further information can be found here\nList of available receivers, processors and exporters:\nreveivers\nprocessors\nexporters\nIn addition, extensions (Health Check, Performance Profiler, zPages) are provided that can be added to the collector to extend the primary functionality of the collector. These do not require direct access to telemetry data and enable additional functionality outside the usual pipeline.\nRelated documentations\nOpenTelemetry Collector\nOpenTelemetry Java documentation\nJaeger\nZipkin\nVictoriaMetrics\nGrafana\nRelated Architectures and Alternatives\nopenTracing\nopenCensus\n"},{"dirname":"provisioning","id":8,"path":"target/generated-docs/solutions/provisioning/index.html","type":"solution","title":"Provisioning","body":"\nProvisioning\nContext &amp; Problem\nThis document describes patterns to automate the deployment of an application to a target environment. The major players in this documentation are (1) the process as a whole (aka provisioning), (2) automation code and (3) application/ infra code to be deployed.\nAutomation code:\nThe automation code is about automating parts of deployment cycle. In an ideal world this covers the entire code development cycle from the first pull request to the final deployment. Possible activities in this cycle are: (1) quality gates, (2) build and (3) deployment.\nQuality gates refer to testing or approval. Approval might include a manual approval to deploy to sensitive environments like production or enforcing a review before the commit through a pull request. Failing a quality gate should stop the workflow to proceed. Ideally they are maximizing application code coverage and kick in as early as possible.\nFrom a timeline perspective major events that are kicking off actions from the automation code are:\nPull request/ commit (Only triggers quality gates that don’t take too long)\nBuild\nThey can be manually, scheduled (e.g. as part of a nightly build) or automatically started. Additional quality gates ensure code quality.\nDeplyoment to target environment\nA typical quality gate for sensible environments such as production are manual approvals.\nThe major construct is a pipeline that implements a certain activity or a combination. The trigger defines the condition that kick offs a pipeline. Kicking off a pipeline usually includes parameters such as the name of the target environment. The support of different triggers is essental to cover the entire lifecycle. Pipelines can be implemented using a UI driven or programatic approach.\nPipelines are built in a modular way which also adds intermediate steps such as placing the built output in a build artefact repository for later deployment. It also introduces the need to orchestrate them into larger workflows such as creating an entire environment from scratch.\nPipelines must ensure traceability of the performed actions across the entire chain including source code repo, the targeted environment or intermediate stores. This includes a versioning schema for built artefacts. Branches in source repos must be tagged to be able to reconstruct the code behind a versioned artefact.\nApplication code/ infra:\nThe base for automation code activities is the code creating/ updating the infrastructure and the application on top. Focus in this documentation is the interaction between automation code and application/ infra code. This includes:\nProgramming approach (Language or UI) for deploying infrastructure and application code\nInteraction standard problems between automation code and application/ infra code\nProblems also depend on the chosen programing approach.\nNot relevant are:\nApplication code: Structuring in repo, programming language specifics and details of build mechanism\nSpecific deployment options of the platform services forming the infrastructure\nProvisioning\nThe process must work in a compliant way end-to-end. Compliance includes the general concern security and to adhere to constraints from the organization’s perspective that need to be enforced.\nProvisoning should be subject to monitoring and must be able to adapt to the organizational structure.\nConfiguration is also a cross concern that affects both sides. From the automation side this includes properties of the environment and settings of the involved automation code. The application specific settings are out of scope in this pattern description. However, provisioning must enable the application code to access settings.\nAs shown automation does not only include pipelines but also additional configuration settings. Automation code must be able to access the infra/ app code for deployment. This pattern includes guidelines regarding options to store the code and how to structure it in a repository. Application code structure is treated as black box.\nIn most cases enterprises have already existing technologies in place that might not be based on cloud of a certain provider. Integration with other third party offerings to cover functionally also from elsewhere is also important.\nThe picture below summarizes the major aspects:\nStandard Problems\nThe following standard problems will be addressed in subsequent paragraphs:\nAutomation code\nRegarding pipelines the folloqing aspects will be detailed:\nModeling environments\nPipeline Implementation (Programing approaches, Triggers, Paramterization, Store Code, Quality Gates)\nOrchestration\nTraceability (Versioning, Tagging)\nInfra/ Application code (Programming approach,\nInteraction standard problems)\nProvisioning\nOrganizational Mapping\nIntegration\nCode Repository (Automation/ Infra &amp; App Code)\nConfiguration\nCompliance\nFor the following aspectcs check out the other defined patterns:\nMonitoring infrastructure and application code\nGeneral guidelines for structuring repositories (e.g. mono vs. multi-repo)\nGeneral guidelines for defining landing zones\nAzure as Platform\nAutomation code\nModelling Environments\nAzure provides the following structural elements to model an environment:\nResource groups: Smallest possible container\nSubscriptions: One subscription can contain many resource groups. With a subscription discounts for dev/ test environments are possible.\nManagement groups: One management group can contain many resource groups. They can be used enforce policies across subscriptions if environments share common characteristics.\nAn environment can be linked to another environment. Linking an environment to multiple environment is beneficial for addressing cross concerns such as monitoring (Similar to Hub/ Spoke topology in networks).\nPipeline Implementation\nThe programming approach can be either UI driven or programmatic. Pipeline programming languages such as YAML structure the actions to be performed by the pipeline and provide basic mechansims such as downloading code from the repo, parameter handling, stating triggers and triggering other programming languages. These other languages are then used to setup infrastructure such as terraform or deploying application code.\nAzure allows to trigger pipelines upon (1) a push to repo, (2) a pull request to repo, (3) a schedule and (4) a pipeline completion (Link).\nThe platform allows to pass parameters by various mechanisms to pipelines(Explicit per user input, programmatically). Parameters can be passed by group identifier or explicitly as key value pairs. Complex structured objects as known from object programming languages are not directly possible (Require parsing of files with object structure). Parametrization might be constrained by the used service in certain areas.\nThe platform provides support for quality gates as follows:\nStatic code analysis\nMicrosoft does not provide own tools for static code analysis but allows integration of others.\nAutomated tests (Unit, Integration, End-To-End)\nMicrosoft provides services that include test management e.g. creating test suites with test cases and getting an overview about the results.\nApproval\nAzure services support approval for a certain environments and enforcing pull requests as quality gates.\nThe Azure platform provides the following basic options to store automation code:\nServices that provide repositories\nIntegration of various external code repositories\nOrchestration\nTo orchestrate pipelines the two following basic mechanisms can be used:\nImplicit Chaining\nIn that case the complete workflow is not explicitly coded in a dedicated pipeline. Pipelines are chained implicitly by triggering events. The biggest problem with that approach is the missing single pane of control. The current state in the overall workflow is for instance only implicitly given by the currently running pipeline.\nCreating a dedicated orchestration pipeline\nAn additional pipeline triggers in this scenario other pipelines acting as building blocks. Pipelines can run separately (Just run the deployment) or as part of a bigger workflow (=create environment from scratch).\nOrchestration must take dependencies into account. They might result from the deployed code or the scope of the pipeline (Scope = e.g. a single microservice; Code = libraries needed).\nOrchestrated pipelines must pass data between them. The recommended method is to use key vault.\nRecreation of resources in short intervals can cause pipelines to fail since the previously deleted resource still exists in the background.(Even although soft delete is not applicable). Whether Azure really deleted everything depends on the service. For instance Azure API management seemed to be affected by that problem.\nTraceability\nTraceability requires an identifier for referencing artefacts. A standard schema is a semantic version. The platform only supports partial support for number generation such as incrementing numbers (Link). Linking the code in the repo to a certain version depends on used repository.\nInfrastructure/ Application code\nA programming language is either \"declarative\" or \"imperative\". Declarative programming languages state the target state and it is the job of the declarative programming language how to get there. The following rules are applied to achieve that: (1) Create a resource if not there, (2) update an existing resource if different properties, (3) delete resource if not there. Imperative programming languages state the how. The internal delta calculation needs to be explicitly programmed here. If possible declarative programming languages are recommended due to automatic delta calculation. Typical case is infrastructure.\nTypical declarative options are shown in detail in the table below. The overall recommendation is to go for terraform. Major reasons for downvoting Bicep/ ARM:\nARM: difficult readability for humans\nBicep: Lack of support for testing based on plan and testing ecosystem since first added recently.\nTable with declarative programming language options:\nCriteria\nBicep\nARM\nTerraform\nSame syntax across clouds\n- (Azure Only)\n- (Azure Only)\n+ (multi)\nWhat if\no (no complete prop list;only display of plan; unexpected delete)\n- (not available)\n+ (plan command)\nDetection current\no (Real anaylsis but time)\n+ (Real anaylsis)\no (Statefile)\nTesting/ static analysis\no (Only via ARM)\n+ (available)\n+ (available)\nHuman Readability\n+\n-\n+\nReverse Engineering\n- (Extra ARM step + adjust)\no (adjust)\n+ (Direct via Terraformer)\nLatest features\no (No embedded fallback)\n+ (native)\no (Time lag but embedded fallback)\nThe major options for imperative programming languages are Azure CLI, Powershell (Windows) or Linux based scripting. Azure CLI is recommended as prefered choice since it works on linux and windows based VMs.\nThe created resources should follow a uniform naming schema. This requires naming to be factored out in a centralized module. Terraform supports factoring out common code in modules. However the backend must already exist and should also follow a naming convention. The recommendation is therefore to expose the common terraform module via an additional path that does not require a backend to determine the names for the azure resources representing the backend.\nProvisioning\nOrganizational Mapping\nThe provisioning must match the organizational requirements of your organization. Azure provides services to model sub units within your organization such as departments, projects and teams.\nIntegration\nPlatform allows a modular approach to outsource certain functionality to third party software such as code repository. Which parts is service specific.\nExternal tools providing pipelines can be integrated in two conceptual ways:\nTrigger automation pipelines from external: This involves the configuration of a CI pipeline in the external tool such as Jenkins and mechanism in the automation service that invokes the CI process when source code is pushed to a repository or a branch.\nRun external pipelines from within the platform: In this approach automation reaches out to an external tool to work with the results.\nConfiguration\nConfiguration for provisioning is required in various areas:\nEnvironment: E.g. name of resource group per potential target environment\nRepository: E.g. relevant repos/ branching\nPipelines: Parameters pipelines run with such as the technical user name or settings required by the built/ deployed code.\nConcrete features used for the above three points depend on the used services. A general storage for sensitive data (Keys, secrets, certificates) in Azure is always Azure Key Vault.\nCompliance\nThe standard concept for role-based access controls is called RBAC in Azure. It assigns principals (=humans or technical accounts) permissions for a certain resource. Regarding provisioning the following users are relevant:\nTechnical user (=service principal) the pipelines are running with\nUsers for administrating the provisioning service\nAzure Active Directory is the central service in Azure that defines and controls all principals (human/ service).\nGranularity of roles that can be granted depend on the service. The boundaries in which users exist/ permissions can be assigned is also service specific.\nSolution (Full blown productive)\nOverview\nThe Azure service targeting a full-blown productive provisioning setup is Azure DevOps.\nNote: Azure DevOps will be superseeded by GitHub in the long run after Microsoft acquired GitHub. New features will be initially implemented there.\nThe services that (can) complement Azure DevOps:\nAzure Key Vault for storing secrets/ exchange of settings\nAzure App Configuration\nThis service provides settings (key-value pairs) and feature toggles. Native integrations exist for typical application programming languages like .NET/ Java. However native integrations with terraform do not exist and it is also a special hardened service for sensitive information as key vault. Therefore, it is recommended to use that service as special case for application layer if feature toggles are needed.\nAzure AD\nAzure Active Directory provides the service principal the pipelines run with.\nMonitoring\nAzure DevOps generates metrics to check the health pipelines and displays te state in the Azure DevOps portal . However no built-in forwarding to App Insights independent from the deployed application exists. Continous monitoring assumes Web Applications.\nCheck out the pattern monitoring how monitoring for infrastructure/ application code can be achieved.\nEssential is that a monitoring consumer gets a single control plane across multiple environments.\nStructural elements to model environments\nThe picture illustrates the setup with the major dependencies:\nPattern Details\nGeting Started\nMany aspects influence the setup of the service. Following a top down approach the following decisions have to be made:\n* Define landing zone of the service itself (out of scope)\n* Organizational mapping\n+\nThis yields the structural components to host provisioning which will be detailed in the next chapter. It introduces the possible components and guidelines for its structuring.\n* Modelling other outlined aspects across automation, infra/ app code and provisioning\nThe structural components are organizations, teams and projects. A team is a unit that supports many team-configurable tools. These tools help you plan and manage work, and make collaboration easier. Every team owns their own backlog, to create a new backlog you create a new team. By configuring teams and backlogs into a hierarchical structure, program owners can more easily track progress across teams, manage portfolios, and generate rollup data.\nA project in Azure DevOps contains the following set of features:\nBoards and backlogs for agile planning\nPipelines for continuous integration and deployment\nRepos\nThe service comes with hosted git repositories inside that service. You can also use the following external source repositories: Bitbuckt Cloud, GitHub, Any generic git repo, Subversion\nTesting\nAzure DevOps supports the following testing by defining test suites with test cases (Link):\nPlanned manual testing. Manual testing by organizing tests into test plans and test suites by designated testers and test leads.\nUser acceptance testing. Testing carried out by designated user acceptance testers to verify the value delivered meets customer requirements, while reusing the test artifacts created by engineering teams.\nExploratory testing. Testing carried out by development teams, including developers, testers, UX teams, product owners and more, by exploring the software systems without using test plans or test suites.\nStakeholder feedback. Testing carried out by stakeholders outside the development team, such as users from marketing and sales divisions.\nTests can also be integrated in pipelines. Pipelines support a wide range of frameworks/ libraries.\nEach organization contains one or more projects\nYour business structure should act as a guide for the number of organizations, projects, and teams that you create in Azure DevOps (Link). Each organization gets its own free tier of services (up to five users for each service type) as follows. You can use all the services, or choose just what you need to complement your existing workflows.\nAzure Pipelines: One hosted job with 1,800 minutes per month for CI/CD and one self-hosted job\nAzure Boards: Work item tracking and Kanban boards\nAzure Repos: for version control and management of source code and artifacts\nAzure Artifacts: Package management\nTesting: Continuous test integration throughout the project life cycle\nAdding multiple projects makes sense in the following cases (Link):\nTo prohibit or manage access to the information contained within a project to select groups\nTo support custom work tracking processes for specific business units within your organization\nTo support entirely separate business units that have their own administrative policies and administrators\nTo support testing customization activities or adding extensions before rolling out changes to the working project\nTo support an Open Source Software (OSS) project\nAdding teams instead of projects is recommended over projects for the following reasons (Link):\nVisibility: It’s much easier to view progress across all teams\nTracking and auditing: It’s easier to link work items and other objects for tracking and auditing purposes\nMaintainability: You minimize the maintenance of security groups and process updates.\nThe table below lists typical configurations along with their characteristics :\nCriteria\n1 project, N teams\n1 org, N projects/ teams\nN orgs\nGeneral guidance\nSmaller or larger organizations with highly aligned teams\nGood when different efforts require different processes (multi)\nLegacy migration\nProcess\nAligned processes across teams; team flexibility to customize boards, dashboards, and so on\nDifferent processes per prj;e.g. different work item types, custom fields\nsame as many projects\nRemaining goals (Automation Code)\nThis chapter details how the above conceptual features can be achieved with Azure DevOps pipelines.\nThe pipeline programming approach can be either UI driven or programmatic by using YAML. YAML organizes pipelines into a hierarchy of stages, jobs and tasks. Tasks are the workhorse where activities are implemented. Tasks support scripting languages as stated below. They in turn allow to install additional libraries frameworks from third party providers such as terraform (or you use extensions that give you additional task types). The list below highlights a few YAML points you have to be aware of:\nPassing files/ artefacts between jobs/ pipelines\nPassing between jobs within the same pipeline requires publishing the files as pipeline artefacts and downloading it afterwards. Passing between syntax requires a different syntax and also requires a version.\nVariables\nVariables can have different scopes. A special syntax is required to publish them at runtime and to consume them in a different job (requires declaration). (Link)\nObtaining client secret\nScripting languages such as terraform might require the client secret for embedded scripting blocks. However, terraform does not provide a way to get it. The only way was to include an AzureCLI scripting task. Setting the argument \"addSpnToEnvironment\" to true makes the value for scripting languages as environment variable. A script can then publish the variable so that the value is available in the YAML pipeline.\nPipelines that shall be triggered by pushing to the repo state in the trigger element the details like branch when they shall run.\nThe example below shows a scheduled trigger:\n# Disable all other triggers\npr: none\ntrigger: none\n# Define schedule\nschedules:\n# Note: Azure DevOps only understands the limited part of the cron\n# expression below. See this link for further details:\n# https://docs.microsoft.com/en-us/azure/devops/pipelines/process/scheduled-triggers?view=azure-devops&amp;tabs=yaml\n# Note: With DevOps organization setting of UTC+1 Berlin,...\n# for a given hour x you have to specify x-2 e.g. 16:00 will be\n# started 18:00 o'clock\n- cron: \"30 5 * * MON,TUE,WED,THU,FRI\"\ndisplayName: Business daily morning creation\nalways: true # also run if no code changes\nbranches:\ninclude:\n- 'refs/heads/master'\nPull request (PR) triggers cause a pipeline to run whenever a pull request is opened with one of the specified target branches, or when changes are pushed to such a pull request. In Azure Repos Git, this functionality is implemented using branch policies. To enable pull request validation in Azure Git Repos, navigate to the branch policies for the desired branch, and configure the Build validation policy for that branch. For more information, see Configure branch policies. Draft pull requests do not trigger a pipeline even if you configure a branch policy. Building pull requests from Azure Repos forks is no different from building pull requests within the same repository or project. You can create forks only within the same organization that your project is part of. (Link)\nTo trigger a pipeline upon the completion of another pipeline, specify the triggering pipeline as a pipeline resource. The following example has two pipelines - app-ci (the pipeline defined by the YAML snippet), and security-lib-ci (the triggering pipeline referenced by the pipeline resource). We want the app-ci pipeline to run automatically every time a new version of security-lib-ci is built.\n# this is being defined in app-ci pipeline\nresources:\npipelines:\n- pipeline: securitylib # Name of the pipeline resource\nsource: security-lib-ci # Name of the pipeline referenced by the pipeline resource\nproject: FabrikamProject # Required only if the source pipeline is in another project\ntrigger: true # Run app-ci pipeline when any run of security-lib-ci completes\nImplicit Chaining for orchestration is possible by using trigger condition. Calling pipelines explicitly is so far only possible with scripting. The code snippet below shows an example:\n#\n# Make call to schedule pipeline run\n#\n# Body\n$body = @{\nstagesToSkip = @()\nresources = @{\nself = @{\nrefName = $branch_name\n}\n}\ntemplateParameters = $params\nvariables = @{}\n}\n$bodyJson = $body | ConvertTo-Json\n# Uri extracted from the Azure DevOps UI\n# $org_uri and $prj_id contain names of current organization/ project\n# $pl_id denotes the internal pipeline id to be started\n$uri = \"${org_uri}${prj_id}/_apis/pipelines/${pl_id}/runs?api-version=5.1-preview.1\"\n# Output paramters\nWrite-Host(\"-------- Call ${pl_name} --------\")\nWrite-Host(\"Headers: ${headersJson}\")\nWrite-Host(\"Json body: ${bodyJson}\")\nWrite-Host(\"Uri: ${uri}\")\ntry\n{\n# Trigger pipeline\n$result = Invoke-RestMethod -Method POST -Headers $headers -Uri $uri -Body $bodyJson\nWrite-Host(\"Result: ${result}\")\n# Wait until run completed\n$buildid = $result.id\n$start_time = (get-date).ToString('T')\nWrite-Host(\"------------ Loop until ${pl_name} completed --------\")\nWrite-Host(\"started runbuild ${buildid} at ${start_time}\")\n# Uri for checking state\n$uri = \"${org_uri}${prj_id}/_apis/pipelines/${pl_id}/runs/${buildid}?api-version=5.1-preview.1\"\nDo {\nStart-Sleep -Seconds 60\n$current_time = (get-date).ToString('T')\n# Retrieve current state\n$result = Invoke-RestMethod -Method GET -Headers $headers -Uri $uri\n$status = $result.state\nWrite-Host(\"Received state ${status} at ${current_time}...\")\n} until ($status -eq \"completed\")\n# return result\n$pl_run_result = $result.result\nWrite-Host(\"Result: ${pl_run_result}\")\nreturn $pl_run_result\n}\ncatch {\n$excMsg = $_.Exception.Message\nWrite-Host(\"Exception text: ${excMsg}\")\nreturn \"Failed\"\n}\nOrchestration must take dependencies into account. They might result from the deployed code or the scope of the pipeline (Scope = e.g. a single microservice; Code = libraries needed).\nOrchestrated pipelines must pass data between them. The recommended method is to use key vault.\nRecreation of resources in short intervals might cause pipelines to fail.Even if resources are deleted they might still exist in the background (Even although soft delete is not applicable). Programming languages can therefore get confused if pipelines recreate things in short intervals. Creating a new resource group can solve the problem since they are part of the tecnical resource id.\nAs part of the configuration Azure DevOps provides the possibility to provide various settings that are used for development such as enforcing pull requests instead of direct pushes to the repo.\nThe major configuration mechanisms in YAML are variables, parameters and variable groups. Variable groups bundle multiple settings as key value pairs. Parameters are not possible in a variable section (Dynamic inclusion of variable groups is possible via file switching). If they are declared on top level they have to be passed when the pipeline is called programmatically or manually by the user.\nQuality gates can be enforced as follows:\nStatic code analysis:\nVarious tool support exists depending on the programming language.\nAutomated tests (Unit, Integration, End-To-End)\nTests can be included in pipelines via additional libraries and additional previous installment through scripting. The task below uses an Azure CLI task to run tests for terraform:\n- task: AzureCLI@2\ndisplayName: Run terratest\ninputs:\nazureSubscription: ${{parameters.svcConn}}\nscriptType: bash\nscriptLocation: 'inlineScript'\naddSpnToEnvironment: true\ninlineScript: |\n# Expose required settings as environment variables\n# ARM_XXX initialized by task due to addSpnToEnvironment = true\nsubsid=`az account show --query id -o tsv`\necho \"client_id:\"$servicePrincipalId\necho \"client_secret:\"$servicePrincipalKey\necho \"subscription_id:\"$subsid\necho \"tenant_id:\"$tenantId\nexport ARM_SUBSCRIPTION_ID=$subsid\nexport ARM_CLIENT_ID=$servicePrincipalId\nexport ARM_CLIENT_SECRET=$servicePrincipalKey\nexport ARM_TENANT_ID=$tenantId\n# Backend settings\nexport storage_account_name=${{parameters.bkStname}}\nexport container_name=${{parameters.bkCntName}}\nexport key=${{parameters.bkRmKeyName}}\n# Other settings\nexport resource_group_name=${{parameters.rgName}}\n# Switch to directory with tests\npwd\ncd test\n# Testfile must end with \"&lt;your name&gt;_test.go\"\ngo test -v my_test.go\nManual approval e.g. for production\nYAML allows deployments to named environments. Approvers can then be defined for the named environments in the portal what causes the deployment pipeline to wait. However Approval must be done multiple times if you have multiple deplyoment blocks. The example below shows a deployment to the environment \"env-demo\":\njobs:\n- deployment:\ndisplayName: run deploy template\npool:\nvmImage: 'ubuntu-latest'\nenvironment: env-demo\nstrategy:\nrunOnce:\ndeploy:\nsteps:\n# - 1. Download artefact\n- task: DownloadPipelineArtifact@2\ndisplayName: Get artefact\ninputs:\ndownloadPath: '$(build.artifactstagingdirectory)'\nartifact: ${{parameters.pipelineArtifactName}}\nRemaining goals (Provisioning)\nConfiguration settings can be broken down into key value pairs. As already stated key vault is the recommended place for storage. Azure App Configuration and variable groups can reference values in Key Vault. Key Value pairs must be selected in YAML based on the target environment. Switching based on the parameter value is possible by constructing filenames based on the parameter value. The resolved filenam contains then the variable group or the key value pairs. as shown below:\n(1) Main pipeline that requires switching\n...\n# Switch in the pipeline which is implemented in a shared repository\nvariables:\n- template: ./pipelines/configurations/vars-env-single-template.yaml@repo-shared\nparameters: ${{parameters.envName}}\n...\n(2) Shared: Switch to correct configuration file\n...\nparameters:\n- name: envName\ndisplayName: name of environment\ntype: string\n# Load filename with resolved parameter value\nvariables:\n- template: vars-env-def-${{parameters.envName}}-template.yaml\n(3) Shared: Configuration file vars-env-def-dev1.yaml\nvariables:\nenvNamePPE1MainScriptLocation: app/dev\nenvNamePPE1SvcLevel: Full\nenvNamePPE1BranchName: dev\nenvNamePPE1KvEnvName: $(envNameCRST)1\nAzure DevOps can integrate with various external tools. Pipelines can be called from external and allow calling external tools. Various third party tools can be manually installed or used via extensions. The built in repository and the artefact store can be replaced with third party tools such as github.\nFor compliance Azure DevOps provides various settings inside Azure DevOps itself and via Azure Active Directory.\nPortal access to Boards, Repos, Pipelines, Artifacts and Test Plans can be controlled through Azure DevOps project settings (Link).\nAzure DevOps supports the following autthentication mechanisms to connect to services and resources in your organization (Link):\nOAuth to generate tokens for accessing REST APIs for Azure DevOps. The Organizations and Profiles APIs support only OAuth.\nSSH authentication to generate encryption keys for using Linux, macOS, and Windows running Git for Windows, but you can’t use Git credential managers or personal access tokens (PATs) for HTTPS authentication.\nPersonal access token (PAT) to generate tokens for:\nAccessing specific resources or activities, like builds or work items\nClients like Xcode and NuGet that require usernames and passwords as basic credentials and don’t support Microsoft account and Azure Active Directory features like multi-factor authentication\nAccessing REST APIs for Azure DevOps\nUser permissions for team members are split in access levels and project permissions inside Azure DevOps. The Basic and higher access levels support full access to all Azure Boards features. Stakeholder access provides partial support to select features, allowing users to view and modify work items without having access to all other features. Additional restrictions are possible by Azure Active Directory settings using conditional access policies and MFA. Azure DevOps honors all conditional access policies 100% for our Web flows. For third-party client flow, like using a PAT with git.exe, IP fencing policies are supported only (no support for MFA policies).\nPermissions to work with repositories can be set under project’s repositories settings which also allows to disable forks. Many forks makes it hard to keep the overview and forking allows to download code into someones private account. Azure DevOps supports creating branch protection policies, which protect the code committed to the main branches (project settings ⇒ repo ⇒ branch policies).\nCompliance affects dealing with sensitive settings. As already stated key vault is the standard service for storing them at runtime. Exports from key vault can only be decrypted in a key vault instance.\nHence, secrets can be stored in a repository in a safe way without having to store the values in plain. Using them later should be done in a safe way. This includes publishing them in a safe way and passing them from YAML to terraform by avoiding log output in plain text. Avoiding log output passing them as environment variables/ files.\nThe following repository structure shows a conceptual breakdown that covers most aspects:\n1. Infra\n1.1. Infrastructure\n1.1.1. Other landing zones\nRepresents other areas with shared functionality that are required. Examples are environments for monitoring, the environment containing Azure DevOps, Key Vault settings etc.\n1.1.2. App Environments\nRepresents the environments where application is deployed to.\n1.1.2.1. Envs\nThis level contains all infrastructure code for seting up en environment. The split between dev and non-dev leverages cost savings for less performant dev environments e.g. by picking cheaper service configurations or totally different Azure services.\n1.1.2.1.1. Dev\n1.1.2.1.2. Non-Dev\n1.1.2.1.3. Modules\nFactored out modules for shared reuse. One example is could a central module to generate the name for a given module.\n1.1.2.2. Envs-Mgmt\nCaptures aspects assumed by the chosen programming language such as terraform for managing an environment. This includes for instance the backend creation code.\n1.2. Pipelines\nPipelines for automating infrastrcuture deployment.\n2. App\n2.1. Application (Black Box)\n2.2. Pipelines\nPipelines for automating app code deployment.\n3. Shared\nCaptures shared aspects between infrastructure and application code such as publishing key vault secrets for a pipeline or triggering another pipeline.\nVariations\nFor Dev/ Test scenarios the following services exist:\nAzure Lab Services (https://docs.microsoft.com/en-us/azure/lab-services/)\nKubernetes\nAzure DevSpaces (Deprecated) in favor of “Bridge-to-kubernetes”\nBridge-to-Kubernetes\nWhen to use\nThis solution assumes that your control plane is in Azure and that your monitored resources are located in Azure.\n"},{"dirname":"security_authentication","id":9,"path":"target/generated-docs/solutions/security_authentication/index.html","type":"solution","title":"Authentication","body":"\nAuthentication\nAccess control is an important aspect for the security in IT application landscapes. There are two different aspects to distinguish:\nAuthentication (Who tries to access?)\nAuthorization (Is the one accessing allowed to do what he wants to do?)\nThis part deals with the recommendations on authentication.\nYou have the following problem to be solved\nIn large IT landscapes it is a highly recommended best-practice to centralize your authentication. Implemeting the actual authentication into every application or service is therefore considered as an anti-pattern. Instead we suggest to use a central identity and access management (IAM) solution based on established products (e.g. Keycloak).\nUsing a central IAM\nWhen using a central IAM, the user is redirected to the identity provider (IdP) when trying to access the application. The IdP returns a login page where the user can log in. After confirming that the user can access the application, the IdP returns an access token that the user can use to access the application. We recommend the use JSON Web Tokens (JWT) within the authentication flow, as this is a widely used method in modern web applications and RESTful services.\nParticipants integrate with the identity provider using protocols such as OpenId Connect, SAMLv2 or WebAuthn. The original incoming request is forwarded to the actual service and the token is added as a bearer token via HTTP header according to OAuth standard. In the application, the token can be validated according to the user’s roles and groups.\nTypically, a gateway is placed in front of the IdP and applications to act as a reverse proxy for the actual service. Incoming traffic goes through the gateway and the gateway is then responsible for authentication through integration with the identity provider. In this way, multiple applications and services can be deployed without implementing integration between the service itself and the IdP. The access token can be validated only on the gateway side or additionally passed to the application for further validation.\nServices are implemented stateless and only accept requests with valid JWT from gateway. When one of your services invokes another service it simply passes on the JWT via HTTP header. This way all sub-sequent invocations happen within the context and with the permissions of the initial user.\nThe gateway should also act as a portal that integrates the UIs of your microservices so that end users do not notice which UI comes from which service, but have the user experience (UX) of a single monolithic UI.\nWhich protocol to use\nWe suggest using OIDC as the protocol for integration with the identity provider. It is easy to integrate and works well with mobile and web-based applications. OIDC uses JSON tokens and RESTful APIs to provide the authentication information. Therefore, it is a much more lightweight solution than SAML, which uses an XML and SOAP-based approach.\nValues for the customer\nall services are independent and decoupled from the actual authentication and IAM\nauthentication can be changed without touching any of your services, only changes need to be made to your gateway(s)\nin large and complex IT landscapes, there may be different requirements for authentication via different channels (e.g. to authenticate internal users via SPNEGO and external users via WebAuthn). In such a case, you can simply set up several variants of your gateway for each channel with different endpoint URLs.\nConventions\nWe recommend the following conventions:\ndefine a short but meaningful unique alphanumeric identifier for each of your services (app-id)\nestablish a clear URL scheme for accessing your apps, e.g. https://gateway.company.com/«app-id»/\nuse a cloud infrastructure platform that allows to manage an overlay network so you can configure loadbalancers or even a service-mesh mapping your service entry points to a consistent URL schema such as https://«app-id»:8443/«app-id»/\nthis way you do not need any configuration or business knowledge inside your gateway as the routing can be implemented fully generic\nuse app-id. as a prefix to all permission groups/roles specific to your service to avoid name clashing in your central IAM\nRelated documentations\ndevon4j authentication guide\ndevon4j JWT guide\nOAuth 2.0\nOpenID Connect\nKeycloak’s securing apps guide\nIAM solutions\nKeycloak\nWSO2\nGluu Server\nForgeRock\n…​\n"},{"dirname":"security_authorization","id":10,"path":"target/generated-docs/solutions/security_authorization/index.html","type":"solution","title":"Authorization","body":"\nAccess control is an important aspect for the security in IT application landscapes. There are two different aspects to distinguish: authentication and authorization.\nThis part deals with the recommendations on authorization, concretely with the general concept and convention. Details how to implement this with specific libraries, or programming-languages are described in the individual stacks of devonfw.\nAuthorization\nDefinition:\nAuthorization is the verification that an authenticated user is allowed to perform the operation he intends to invoke.\nClarification of terms:\nFor clarification we also want to give a common understanding of related terms that have no unique definition and consistent usage in the wild.\nTable 1. Security terms related to authorization\nTerm\nMeaning and comment\nPrincipal\nAn entity that can be authenticated e.g. a user, an application\nPermission\nA permission is an object that allows a principal to perform an operation in the system. This permission can be granted (give) or revoked (taken away). Sometimes people also use the term right what is actually wrong as a right (such as the right to be free) can not be revoked.\nGroup\nWe use the term group in this context for an object that contains permissions. A group may also contain other groups. Then the group represents the set of all recursively contained permissions.\nRole\nWe consider a role as a specific form of group that also contains permissions. A role identifies a specific function of a principal. A user can act in a role.For simple scenarios a principal has a single role associated. In more complex situations a principal can have multiple roles but has only one active role at a time that he can choose out of his assigned roles. For KISS it is sometimes sufficient to avoid this by creating multiple accounts for the few users with multiple roles. Otherwise at least avoid switching roles at run-time in clients as this may cause problems with related states. Simply restart the client with the new role as parameter in case the user wants to switch his role.\nAccess Control\nAny permission, group, role, etc., which declares a control for access management.\nSuggestions on the access model\nFor the access model we give the following suggestions:\nEach Access Control (permission, group, role, …​) is uniquely identified by a human readable string.\nWe create a unique permission for each use-case.\nWe define groups that combine permissions to typical and useful sets for the users.\nWe define roles as specific groups as required by our business demands.\nWe allow to associate users with a list of Access Controls.\nFor authorization of an implemented use case we determine the required permission. Furthermore, we determine the current user and verify that the required permission is contained in the tree spanned by all his associated Access Controls. If the user does not have the permission we throw a security exception and thus abort the operation and transaction.\nWe avoid negative permissions, that is a user has no permission by default and only those granted to him explicitly give him additional permission for specific things. Permissions granted can not be reduced by other permissions.\nTechnically we consider permissions as a secret of the application. Administrators shall not fiddle with individual permissions but grant them via groups. So the access management provides a list of strings identifying the Access Controls of a user. The individual application itself contains these Access Controls in a structured way, whereas each group forms a permission tree.\nDo not use the pattern that defines non-configured permission as no limitation or in other word all permissions.\n[DB1,DB2] → allow to access DB1 and DB2\n[] → have no permission at all → good\n[] → have all permissions → bad\nNaming conventions\nAs stated above each Access Control is uniquely identified by a human readable string. This string should follow the naming convention:\n«app-id».«local-name»\nFor Access Control Permissions the «local-name» again follows the convention:\n«verb»«object»\nThe segments are defined by the following table:\nTable 2. Segments of Access Control Permission ID\nSegment\nDescription\nExample\n«app-id»\nIs a unique technical but human readable string of the application (or microservice). It shall not contain special characters and especially no dot or whitespace. We recommend to use lower-train-case-ascii-syntax. The identity and access management should be organized on enterprise level rather than application level. Therefore permissions of different apps might easily clash (e.g. two apps might both define a group ReadMasterData but some user shall get this group for only one of these two apps). Using the «app-id». prefix is a simple but powerful namespacing concept that allows you to scale and grow. You may also reserve specific «app-id»s for cross-cutting concerns that do not actually reflect a single app e.g to grant access to a geographic region.\nshop\n«verb»\nThe action that is to be performed on «object». We use Find for searching and reading data. Save shall be used both for create and update. Only if you really have demands to separate these two you may use Create in addition to Save. Finally, Delete is used for deletions. For non CRUD actions you are free to use additional verbs such as Approve or Reject.\nFind\n«object»\nThe affected object or entity. Shall be named according to your data-model\nProduct\nSo as an example shop.FindProduct will reflect the permission to search and retrieve a Product in the shop application. The group shop.ReadMasterData may combine all permissions to read master-data from the shop. However, also a group shop.Admin may exist for the Admin role of the shop application. Here the «local-name» is Admin that does not follow the «verb»«object» schema.\nData permissions\nIn some projects there are demands for permissions and authorization that is dependent on the processed data. E.g. a user may only be allowed to read or write data for a specific region. This is adding some additional complexity to your authorization. If you can avoid this it is always best to keep things simple. However, in various cases this is a requirement. Please clarify the following questions before you make your decisions how to design your access controls:\nDo you need to separate data-permissions independent of the functional permissions? E.g. may it be required to express that a user can read data from the countries ES and PL but is only permitted to modify data from PL? In such case a single assignment of \"country-permissions\" to users is insufficient.\nDo you want to grant data-permissions individually for each application (higher flexibility and complexity) or for the entire application landscape (simplicity, better maintenance for administrators)? In case of the first approach you would rather have access controls like app1.country.GB and app2.country.GB.\nDo your data-permissions depend on objects that can be created dynamically inside your application?\nIf you want to grant data-permissions on other business objects (entities), how do you want to reference them (primary keys, business keys, etc.)? What reference is most stable? Which is most readable?\nIf data-permission is the way to go, please checkout our guidance and patterns how to solve this properly.\nImplementation hints\nAccess control\nData permission\n"}]