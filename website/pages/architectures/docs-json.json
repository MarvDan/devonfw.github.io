[{"dirname":"architecture_hexgagonal","id":0,"path":"target/generated-docs/solutions/architecture_hexgagonal/index.html","type":"solution","title":"Hexagonal architecture","body":"\nHexagonal architecture\nThis article explains so-called Hexagonal Architecture which is one of architectural styles applicable for wide variety of applications / software.\nYou have the following problem to be solved\nWe need to develop our software in more robust way so that it can survive technological changes.\nYou should be not forced to rewrite big amount of business logic code just because you need to:\nreplace specific transport layer protocol,\nor replace storage technology,\nor you want to migrate from one cloud provider to another cloud provider,\nor …​\nEventually, you don’t want your domain code base to be scattered and obscured with technological details such as HTTP exceptions, SQL code, JPA/Spring annotations, etc.\nThe proposed solution enables the customer\nWith appropriate architecture that is technology agnostic the customer should expect:\npredictable and limited costs of migration to newer version of technological components (including \"breaking changes\" migrations),\nmanageable costs of replacing technological components (such as exchanging SQL provider or migrating from Springboot to Quarkus),\nmanageable costs of infrastructure shifts (such as changing cloud provider),\nlower overall maintenance costs due to cleaner domain code and higher software quality.\nThe proposed solution is\naddressing your business\nThis solution is most valuable for medium- to high-complexity software systems.\nExpected benefits are measured as further development and maintenance costs as presented on the following pseudo-graph by Martin Fowler (see bibliography for source).\nHexagonal architecture can be applied to any business domain, however it works best with domains that have rich business rules.\ndefinitively needed if\nHexagonal architecture is preferred solution if:\nthere is a high probability of exchanging certain technical component in the future,\nthere is a necessity of applying a DDD technique (Domain Driven Design).\nHow to solve your problem\nBe domain centric\nPlan and implement your domain model without any technical dependencies.\nUse standard features of your programming language as they are sufficient to realize your goals.\nYou can use power of object oriented programming and model entities or aggregates with methods mutating their state.\nBe use case centric\nPlan and implement your use cases without any technical dependencies.\nUse domain model as the only dependency to your use cases.\nThe use case is basically a function that uses and interacts with domain model.\nThe domain and use cases form \"the core\" as shown in the center of following diagram.\nIsolate and protect your core\nKeep your domain model and use cases isolated from technical details.\nAs each use case is basically a function, call this function from glue code called inbound adapter.\nThe interface between your use case and the adapter is called inbound port.\nConvert data from inbound specific form into use case specific form if necessary.\nIf your use case must call any external system (that is usually another API, event queue, database), use inversion of control pattern and hide that external system behind interface.\nWe call such interface an outbound port.\nManage your dependencies\nYour ultimate goal is to keep your system core clean of any technical dependencies.\nIt makes more sense to present such architecture using concentric circles rather than stacked layers.\nWith such visualisation in mind you have to ensure that dependencies always point inwards and never in opposite direction (refer dashed arrows on following diagram).\nIt is important to note, that layers inside the core, as visible in \"onion\" model are optional.\nYou can put as many layers as you want, depending on needs.\nThe bare minimum seems to be however: domain and use cases.\nUse appropriate tooling\nTo practically manage dependencies you can use:\nmultiple source modules (using Maven or Gradle),\nArchUnit,\ncombine both if needed.\nGo beyond!\nRelated documentation\nThe Hexagonal Architecture by Alistair Cockburn\nThe Clean Architecture by Robert C. Martin\nThe Onion Architecture by Jeffrey Palermon\nClean Architecture: A Craftsman’s Guide to Software Structure and Design, Robert C. Martin\nGet your hands dirty on clean architecture, Tom Hombergs\nIs quality worth costs by Martin Fowler\nPractical example by Herberto Graca\nHex Thai Star - reimplementation of My-Thai-Star (WiP)\nReady for changes with Hexagonal Architecture by Damir Svrtan and Sergii Makagon\n"},{"dirname":"cloud_storage_overview","id":1,"path":"target/generated-docs/solutions/cloud_storage_overview/index.html","type":"solution","title":"Cloud Storage","body":"\nCloud Storage\nThis article mainly focuses on below topics:\n- What is Cloud Storage?\n- How does it works?\n- Benefits of Cloud Storage\n- Cloud Storage requirement\n- Types of Cloud Storage\n- Product and services offered by different vendors\nWhat is Cloud Storage?\nCloud Storage is a cloud computing model in which data is transmitted and stored on remote storage systems, where it is maintained, managed, backed up and made available to users over a network — typically, the internet.\nIt’s delivered on demand with just-in-time capacity and costs, and eliminates buying and managing your own data storage infrastructure. This gives you agility, global scale and durability, with “anytime, anywhere” data access.\nHow does it works?\nCloud Storage is purchased from a third party cloud vendor who owns and operates data storage capacity and delivers it over the Internet in a pay-as-you-go model. Typically, you connect to the storage cloud either through the internet or a dedicated private connection, using a web portal, website, or a mobile app. The server with which you connect forwards your data to a pool of servers located in one or more data centers, depending on the size of the cloud provider’s operation.Applications access Cloud Storage through traditional storage protocols or directly via an API.\nThese Cloud Storage vendors manage capacity, security and durability to make data accessible to your applications all around the world.\nCloud Storage is available in private, public and hybrid clouds.\nPublic Cloud Storage: In this model, you connect over the internet to a storage cloud that’s maintained by a cloud provider and used by other companies.\nPrivate Cloud Storage: Private Cloud Storage setups typically replicate the cloud model, but they reside within your network, leveraging a physical server to create instances of virtual servers to increase capacity. You can choose to take full control of an on-premise private cloud or engage a Cloud Storage provider to build a dedicated private cloud that you can access with a private connection.\nHybrid Cloud Storage: This model combines elements of private and public clouds, giving organizations a choice of which data to store in which cloud.\nBenefits of Cloud Storage\nTotal Cost of Ownership:\nWith Cloud Storage, there is no hardware to purchase, storage to provision, or capital being used for \"someday\" scenarios. You can add or remove capacity on demand, quickly change performance and retention characteristics, and only pay for storage that you actually use.\nLess frequently accessed data can even be automatically moved to lower cost tiers in accordance with auditable rules, driving economies of scale.\nTime to Deployment:\nCloud Storage allows IT to quickly deliver the exact amount of storage needed, right when it’s needed. This allows IT to focus on solving complex application problems instead of having to manage storage systems.\nInformation Management:\nCentralizing storage in the cloud creates a tremendous leverage point for new use cases. By using Cloud Storage lifecycle management policies, you can perform powerful information management tasks including automated tiering or locking down data in support of compliance requirements.\nScalability: Growth constraints are one of the most severe limitations of on-premise storage. With Cloud Storage, you can scale up as much as you need. Capacity is virtually unlimited.\nLimitations of Cloud Storage\nSecurity: Security concerns are common with cloud-based services. Cloud Storage providers try to secure their infrastructure with up-to-date technologies and practices,but occasional breaches have occurred, creating discomfort with users. Security is shared responsibility with cloud providers and users.\nLatency: Delays in data transmission to and from the cloud can occur as a result of traffic congestion, especially when you use shared public internet connections.\nRegulatory compilance: Certain industries, such as healthcare and finance, have to comply with strict data privacy and archival regulations, which may prevent companies from using Cloud Storage for certain types of files, such as medical and investment records.\nType of Cloud Storage\nMajorly, below are main types of Cloud Storage:\nObject Storage: It manages data as objects. Each object includes the data in a file, its associated metadata, and an identifier. Objects store data in the format it arrives in and makes it possible to customize metadata in ways that make the data easier to access and analyze. Instead of being organized in files or folder hierarchies, objects are kept in repositories that deliver virtually unlimited scalability. Since there is no filing hierarchy and the metadata is customizable, object storage allows you to optimize storage resources in a cost-effective way.\nFor example, S3(Simple Storage Service) in AWS, Blob Storage in Azure and Google Cloud Storage in GCP are object storage.\nFile Storage: The file storage method saves data in the hierarchical file and folder structure with which most of us are familiar. The data retains its format, whether residing in the storage system or in the client where it originates, and the hierarchy makes it easier and more intuitive to find and retrieve files when needed. File storage is commonly used for development platforms, home directories, and repositories for video, audio, and other files.\nFor example, EFS and FSx are file storage services in AWS, azure file storage in Azure and Google Cloud Filestore in GCP.\nBlock Storage: Block storage, sometimes referred to as block-level storage, is a technology that is used to store data files on Storage Area Networks (SANs) or cloud-based storage environments. Developers favor block storage for computing situations where they require fast, efficient, and reliable data transportation.Block storage breaks up data into blocks and then stores those blocks as separate pieces, each with a unique identifier. The SAN places those blocks of data wherever it is most efficient.\nBlock storage also decouples data from user environments, allowing that data to be spread across multiple environments. This creates multiple paths to the data and allows the user to retrieve it quickly.\nFor example, EBS in AWS and Google Cloud Persistent Disks in GCP.\nAlso, archival and database services can be considered for data storage\nProducts &amp; Services\nBelow table contains services from different cloud vendors:\nVendor\nStorage Services\nDatabase Services\nBackup Services\nAWS\n• Simple Storage Service (S3)\n• Elastic Block Storage (EBS)\n• Elastic File System (EFS)\n• Storage Gateway\n• Snowball\n• Snowball Edge\n• Snowmobile\n• Aurora\n• RDS\n• DynamoDB\n• ElastiCache\n• Redshift\n• Neptune\nGlacier\nAzure\n• Blob Storage\n• Queue Storage\n• File Storage\n• Disk Storage\n• Data Lake Store\n• SQL Database\n• Database for MySQL\n• Database for PostgreSQL\n• Data Warehouse\n• Server Stretch Database\n• Cosmos DB\n• Table Storage\n• Redis Cache\n• Data Factory\n• Archive Storage\n• Backup\n• Site Recovery\nGCP\n• Cloud Storage\n• Persistent Disk\n• Transfer Appliance\n• Transfer Service\n• Cloud SQL\n• Cloud Bigtable\n• Cloud Spanner\n• Cloud Datastore\nNone\nReferences:\nhttps://www.ibm.com/cloud/learn/cloud-storage#toc-what-is-cl-vt64lltQ\nhttps://aws.amazon.com/what-is-cloud-storage/\n"},{"dirname":"communication","id":2,"path":"target/generated-docs/solutions/communication/index.html","type":"solution","title":"Communication among services","body":"\nCommunication among services\nCommunication is a very important aspect while building IT application, especially large and complex one or one with microservices architecture. Based on your analysis of your problems, considering the following basic questions to start with:\nWhich services have to communicate with which services? to exchange which type of data?\nHow should the communication look like? in a synchronous way or asynchronous way?\nOnce you have your answers, make your choices on concrete protocols, products and libraries. Using multiple of them in your application is fine.\nUnfortunately, there is no silver bullet. Each communication type has its own advantages and disadvantages and target a different scenario and goals. With that said, do not always apply REST or always apply message-driven approaches (e.g. Kafka) just because you have heard some success stories about it.\nHere we try to expose cross-cutting best practices for communication among services in larger IT application landscapes on a high level. Details how to implement this with specific libraries or programming-languages are described in the individual stacks of devonfw.\nCommunication types\nGenerally, types of communication can be classified in two axes.\nThe first axis defines if the protocol is synchronous or asynchronous:\nSynchronous protocol: The client sends a request and waits for a response from the service as the client code can only continue its task when it receives the server response. That’s independent of the client code execution that could be synchronous (thread is blocked) or asynchronous (thread isn’t blocked, and the response will reach a callback eventually). e.g. HTTP/HTTPs\nAsynchronous protocol: The client code or message sender usually doesn’t wait for a response. It just sends the message as when sending a message to a queue or any other message broker. e.g AMQP\nThe second axis defines if the communication has a single receiver or multiple receivers:\nSingle receiver: Each request must be processed by exactly one receiver or service.\nMultiple receivers: Each request can be processed by zero to multiple receivers. This type of communication must be asynchronous. An example is the publish/subscribe mechanism.\nOne common style is single-receiver communication with a synchronous protocol like HTTP/HTTPS when invoking a regular Web API HTTP service. Microservices also typically use messaging protocols for asynchronous communication between microservices. We will discuss more about some popular approaches adopted these days including:\n[rpc]\n[messaging_and_eventing]\n[service_mesh]\nRPC\nWhen it comes to RPC communication nowadays we immediately talk about HTTPS.\nThe most common choice is REST with JSON that perfectly works together with web browsers.\nHowever, please also consider other options especially for backend communication:\ngRPC is an efficient, high performance RPC protocol\n…​\nProtocols like SOAP or even RMI, Corba, etc. should be considered as legacy and discouraged.\nMessaging and eventing\nWe do not even try to distinguish between messaging and eventing.\nA message can be seen as an event and an event as a message.\nThe nature of this communication style is that it is entirely asynchronous and the sender of the event or message does not have to know about the receipient(s).\nThere can even be many recipients for the same event or message.\nTypically there is a central messaging system (event bus) that is all you need to know about to send and receive your events or messages.\nThis leads to a very loose coupling of your services.\nWhile this adds flexibility it also can add quite some complexity to understand what is actually going on.\nDebugging and tracing the communication can get really hard.\nIn the worst case you can lose control and end up with cyclic events triggering each other till eternity.\nHowever, when properly applied, this communication style can make your architecture very powerful and extendable.\nOne of the most common choices for such a messaging system is kafka. Other message brokers to be considered are RabbitMQ and ActiveMQ. In an enterprise environment where often a Oracle Database is in place you might also consider Oracle AQ.\nService mesh\nIn the context of microservices, service mesh is a key to communication.\nService-to-service communication is what makes microservices possible.\nBut creating, routing and managing this communication, both within and across application clusters, becomes increasingly complex as the number of services grows.\nService mesh solves this problem. It takes the logic governing service-to-service communication out of individual services and abstracts it to a layer of infrastructure.\nIt also captures many aspects of the communication as security (e.g. encryption) or performance metrics which can be used to monitor the cluster as well as identify and analyze system failures if occurs.\nOne of the most common and best choices for service mesh is istio.\nIstio is an open-source service mesh with powerful features that enable developers to easily configure\nthe traffic flow\nthe authentication and authorization among internal and to external services and\nmonitoring tools such as kiali, prometheus and grafana.\nCommunication contracts\nA communication contract is a structured document that defines the expected input and output of a service.\nThere are two approaches to using contracts for communications:\nContract first: There is a design document that defines the formal contract of the communication and the code artefacts are derived with some tooling.\nCode first: Code is written directly and documentation is generated from there.\nWhich approach is better depends on your project setup.\nWhile code first often feels more lightweight and brings less over head, for a more complex project setup the formality of the contract could be very beneficial.\nContracts can be used both for synchronous or asynchronous communications, for example Open Api has become the standard for REST contracts and Async Api is growing in adoption for the asynchronous needs.\nLinks\nREST vs messaging for microservices\nEventing and messaging\nKafka microservices\nCommunication in a microservices architecture\nGetting started with istio\nKeycloak and istio\nImplementation hints\nREST:\nJava Server\nJava Client\nAngular\n.NET/C#\nnode.js\nKafka:\nJava\n"},{"dirname":"microsvc_platforms_intro","id":3,"path":"target/generated-docs/solutions/microsvc_platforms_intro/index.html","type":"solution","title":"Microservice Platforms Introduction","body":"\nMicroservice Platforms Introduction\nContext &amp; Problem\nMicroservice orchestration is the automatic process of managing or scheduling the work of individual microservices of an application within multiple clusters. The platform provides an automated process of managing, scaling, and maintaining microservices of an applications.\nThe container orchestration platform kubernetes is the de facto standard in the meantime (Docker swarm and others are ngelectable compared to Kubernetes in the meantime). Containers are executable units of software containing application code, libraries, and dependencies so that the application can be run anywhere.\nContainer orchestration tools automate the management of several tasks that software teams encounter in a container’s lifecycle, including the following: Deployment, Scaling and load balancing/traffic routing, Networking, Insights, Provisioning, Configuration and scheduling, Allocation of resources, Moving to physical hosts, Service discovery, Health monitoring, Cluster management (Link).\nThe picture below summarizes the major aspects:\nStandard Problems\nThe following standard problems will be addressed in subsequent paragraphs:\nOrchestration Platform:\nTo be detailed:\nDeplyoment (of resources into cluster)\nScaling\nload balancing/traffic routing\nNetworking\nConfiguration and scheduling\nService discovery\nApplication services like DAPR\nResources to be deployed:\nTo be detailed:\n* Resources (Containers but also other Kubernetes objects)\n* Building of containers\n* Registry\nContainer Platform:\nAffect containers running on/ platform\nTo be detailed:\n* monitoring\n* Security\n* Provisioning\nFor the following aspects check out the other defined patterns:\nMonitoring infrastructure and application code\nGeneral guidelines for structuring repositories (e.g. mono vs. multi-repo)\nGeneral guidelines for defining landing zones\nAzure as Platform\nAzure provides container platforms that address major concerns:\nCluster orchestration ⇒ What Kubernetes\nService Meshes ⇒ what they cover\nApplication patterns ⇒ DAPR provides application patterns like publish/ subscribe and abstracts from Kubernetes\nAzure Arc ⇒ extends control plane to non Azure hosted Kubernetes clusters (e.g. on-premise, other clouds)\n"},{"dirname":"microsvc_platforms_intro_azure_aks","id":4,"path":"target/generated-docs/solutions/microsvc_platforms_intro_azure_aks/index.html","type":"solution","title":"Azure Microservice Platforms Solution AKS","body":"\nAzure Microservice Platforms Solution AKS\nOverview\nTODO kubernetes core\nThe services that (can) complement Kubernetes:\nProvisioning\nSee Provisioning for general aspects how you can handle provisioing in Azure such as creating pipelines for creating infrastructure. Additional aspects such as building a containe image are described here.\nAzure Container Registry\nAzure AD\nAzure Active Directory provides the service principal the pipelines run with.\nMonitoring\nFor infrastructure monitoring see \"Monitoring\". For application monitoring specific tools exist which will be described here.\nThe picture illustrates the setup with the major dependencies:\nPattern Details\nGeting Started\nTODO if there are options what do I have to decide first e.g. organizations, projects and teams in case of Azure DevOps\nRemaining goals\nPoints from summary Migration:\nDeployment resources to Kubernetes\nInstead of having to write separate YAML files for each application manually, you can simply create a Helm chart and let Helm deploy the application to the cluster for you. Helm charts contain templates for various Kubernetes resources that combine to form an application. A Helm chart can be customized when deploying it on different Kubernetes clusters. Helm charts can be created in such a way that environment or deployment-specific configurations can be extracted out to a separate file so that these values can be specified when the Helm chart is deployed. For example, you need not have separate charts for deploying an application in development, staging, and production environments.\nChart linting is an easy tool that you can add to your pipeline to ensure your deployments are valid and versioned correctly.\nFor security reasons and improvement of Helm charts, it is useful to make use of at least one Helm linting tool.\nWhy choosing Polaris as Linting Tool:\nFor helm chart linting, there are several tools like Polaris, kube-score or config-lint available. With Polaris, checks and rules are already given by default, whereby other tools need a lot of custom rules configuration and are therefore more complex to setup.\nPolaris runs a variety of checks to ensure that Kubernetes pods and controllers are configured using best practices, helping to avoid problems in the future.\nPolaris can be either installed inside a cluster or as a command-line tool to analyze Kubernetes manifests statically.\nThe Helm operator provides an extension to Flux that automates Helm Chart releases. A Helm Chart release is described via a Kubernetes custom resource named HelmRelease. Flux synchronizes these resources from Git to the cluster, while the Helm operator makes sure Helm Charts are released as specified in the resources.\nConfiguration\nAzure Key Vault to Kubernetes (akv2k8s) is used for our applications, to make Azure Key Vault secrets, certificates and keys available to use in a secure way.\nThe goals of Azure Key Vault to Kubernetes are to avoid a direct program dependency for getting secrets, secure and low risk to transfer Azure Key Vault secrets and transparently inject secrets into applications.\nPer default secrets, configurations and certificates can be easily read and accessed by users in Kubernetes and access to them can only be restricted by setting access rights. This will be avoided and is a huge benefit of using akv2k8s with a simple setup of Azure Key Vault and the option to set more detailed restrictions and configurations.\nQuestion: What about telling container what is the right key vault? (Injection needs to know about environment)\nTools for synchronizing configuration:\nArgoCD:\nContinuous Delivery for Kubernetes: ArgoCD is a leader developed by Intuit and synchronizes changes in the code of applications, photos and cluster definitions, so the Git - repository , to the cluster. The solution is open source software, kept relatively simple and is one of the most important and oldest tools on the market.\nFlux\nIntegration of Flux CD into the workflow: Flux basically does the same job as ArgoCD, i.e. synchronizes the repository and cluster in the course of continuous delivery. Flux is also open source and kept simple - the special thing: It comes from the GitOps inventor Weaveworks.\nQuestion: Have heard about built in Flux support in AKS\nMonitoring\nApplication monitoring: Prometheus\nInfrastructure: see Standard (Extend dashboarding)\nVariations\nThe following alternatives exist:\nManaged Openshift\nComment Openshift: Refers to Red Hat Openshift and not hosted offering\nTODO: Better variation for me managed OpenShift (Link)\nCriteria\nOpenshift\nKubernetes\nFlexibility\nLimited; opinionated components/ fnctionalities\ngreater flexibility\nInstallation\nlimited options\nalmost anywhere\nSecurity\nvery strict; certain permission level for maintenance\nDeployment\nless flexible DeplyomentConfig\nvery flexile helm charts\nExternal access\nvia Routers\nvia ingress conntrollers\nManagement\nImageStreams improve management\nManagement container images not that easy\nUsere experience\nBetter user support\nadditional tool for user experience\nNetworking\nnative networking solution\nsome components don’t have networking + third party required\nService Catalog\nWhen to use\nWhen you need orchestration support due to a higher number of microservices. If you start with a single service then Function App reduces greatly the complexity. You can still later on move to Kubernetes by containerizing your function app code.\n"},{"dirname":"monitoring_aws_cloudwatchCustomAlarm","id":5,"path":"target/generated-docs/solutions/monitoring_aws_cloudwatchCustomAlarm/index.html","type":"solution","title":"Customise your cloudwatch alerts","body":"\nTable of Contents\nCustomise your cloudwatch alerts\nYou have the following problem to be solved\nThe proposed solution enables the customer\nRelated Architectures and Alternatives\nProducts &amp; Services\nCustomise your cloudwatch alerts\nYou have the following problem to be solved\nCloudwatch alarms give you the ability to notify you in case an alarm is triggered. The standard message layout of cloudwatch is very inflexible and not customisable. With this approach it is possible to fill HTML templates with further information, links and buttons to find the solution ASAP.\nThe proposed solution enables the customer\nto send out an customised e-mail template which looks more professional and has additional information compared to the standard cloudwatch alarm.\nRelated Architectures and Alternatives\nSee code here: https://github.com/AlessandroVol23/cloudwatch-custom-email-cdk\nProducts &amp; Services\nCloudwatch Alarm: Some cloudwatch alarm\nSNS Topic which will be triggered by the cloudwatch alarm\nLambda which will be called by the SNS topic\nSES will be used by the lambda to send out HTML emails\nAbout\nAbout devonfw\nFeatures\nTechnology Stack\nExplore\nGetting started\nArchitecture\nResources\nDocs\nUser guide\nReleases information\nWiki\nCommunity\nContributors\nWebsite Contribution\nTerms of Use\nSupport\nimport { UtilsModule } from '/website/shared/utils.js';\nimport { HeaderModule } from '/website/components/header/header.js';\nlet searchData = { index: null, documents: null };\nUtilsModule.loadIndex(searchData);\n$(window).on('load', function() {\nconst queryFun = () => {\nHeaderModule.queryFunction(searchData);\n};\nHeaderModule.searchOnClick(queryFun);\n});\nlet bb = document.getElementById('menu-button');\nbb.addEventListener('click', function() {\ndocument.querySelector('.website-navbar ul').classList.toggle('visible');\nconsole.log(document.querySelector('.website-navbar ul'))\n})\nimport { EditLinksModule } from '/website/shared/editlinks.js';\nlet alwaysVisible = true;\nif(document.location.pathname.endsWith(\"pages/welcome/welcome.html\")) {\nalwaysVisible = false;\n}\nEditLinksModule.addEditLinks(alwaysVisible);\nLast updated 2021-10-05 08:39:54 UTC\n"},{"dirname":"monitoring_azure_native","id":6,"path":"target/generated-docs/solutions/monitoring_azure_native/index.html","type":"solution","title":"Monitoring Solution Azure Native","body":"\nTable of Contents\nMonitoring Solution Azure Native\nApplication Monitoring\nInfrastructure Monitoring\nMonitoring Solution Azure Native\nApplication Monitoring\nOverview\nThe solution is to use Azure Monitor with App Insights, Log Analytics and the following platform features regarding the monitored resources. The focus of this chapter is to introduce the relevant features. Recommendations for a concrete setup are given in the next chapter.\nThe relevant Azure monitor features are as follows:\nCollection/ Storage (Monitoring Plane)\nTelemetry can either be stored internally inside the monitoring plane by App Insights/ Log Analytics or externally.\nTelemetry can be pulled from the monitoring plane. This is limited to metrics but faster than pushing. Pushing can be necessary if the telemetry is not available in Azure monitor out of the box or pulling from the monitored resources is not possible. The major mechanisms to push telemetry to the monitoring plane are:\nDiagnostic setting\nApp Insights Instrumentation/ Linking: Linked App Insights must be specified for the monitored resource. Some Azure Services such as Azure App Service come with a built-in App Insight integration. However, other services only provide diagnostic settings instead such as API management.\nManual forwarding: E.g. by scheduled process using the APIs provided by Azure Monitor for App Insights and Log Analytics. A lightweight Azure service for polling services to be monitored is Azure Automation. It allows to host and running scripts.\nApp Insights/ log analytics also provide APIs to manual forward data. However this APIs have some constraints:\ntimestamp cannot be set freely (Both)\ndeleting something is not possible (Both)\nsaved query cannot be updated (App insights only)\nAnalysis/ Diagnosis (Monitoring Plane)\nAzure Monitor comes with no built-in support for KPIs such as code quality, test coverage or availability/ maintenance. However, standard KPIs such as mean time between failure (=MTBF) can be programmed with Kusto queries.\nAzure Application Insights sends web requests to your application at regular intervals from points around the world. It can alert you if your application isn’t responding, or if it responds too slowly as described here.\nKusto queries across multiple application insights or log analytic workspaces are possible. App insight or log analytic workspaces must then be referenced with an additional identifier (App Insights: app('&lt;identifier&gt;'); Log Analytics: (workspace('&lt;qualifier&gt;')) as shown in the samples below. Various options for identifiers exist such as name and guid as described here:\n// Cross-Kusto app insights example\nunion app('mmsportal-prod').requests, app('AI-Prototype/Fabrikam/fabrikamprod').requests, requests\n| summarize count() by bin(timestamp, 1h)\n// Cross-Kusto log analytics example\nunion Update, workspace(\"b438b4f6-912a-46d5-9cb1-b44069212ab4\").Update\n| where TimeGenerated &gt;= ago(1h)\n| where UpdateState == \"Needed\"\n| summarize dcount(Computer) by Classification\nAzure Data Explorer is a service for large scale analysis of telemetry. Large refers large amount of data or high frequency of time series data as described here.\nVisualization/ Alerting (Monitoring Plane)\nNatively Azure monitor provides as dashboarding options (1) Azure dashboards and (2) Azure workbooks.\nAlerts come with the following features:\nTrigger: Results from Kusto queries can be used as trigger.\nAction Groups: Assigning same action (=Action Group) to different triggers\nSmart Groups (Preview as of 24.08.2021): Groups alerts that are triggered simultanously by using artificial intelligence as described here\nAction Rules (Preview as of 24.08.2021): Allows to suppress (e.g. due to maintenance), scope and filter alerts as described here\nReporting: Existing report for SLA/ Outages by using predefined Azure Monitor workbooks from gallery as described here\nApplication Insight comes with the following tools for exploration and root cause analysis:\nApplication Map ⇒ application dependencies in other services such as backend APIs or databases\nSmart Detection ⇒ warn you when anomalies in performance or utilization patterns\nUsage Analysis ⇒ features of your application are most frequently used\nRelease annotations ⇒ visual indicators in your Application Insights charts of new builds and other events. Possible to correlate changes in application performance to code releases.\nCross-component transaction diagnostics ⇒ The unified diagnostics experience automatically correlates server-side telemetry from across all your Application Insights monitored components into a single view. It doesn’t matter if you have multiple resources with separate instrumentation keys. Application Insights detects the underlying relationship and allows you to easily diagnose the application component, dependency, or exception that caused a transaction slowdown or failure as described here.\nSnapshot Debugger ⇒ collect a snapshot of a live application in case of an exception, to analyze it at a later stage.\nCorrelation ⇒ Special fields are provided to convey global identifiers appearing in every request as described here.\nAzure Monitor has also extensive integration features. This includes:\nIntegrating telemetry from other Azure services (e.g. Azure Security Center also forwards to Azure Monitor)\nIntegrating external data sources (e.g. Blobs by using Kusto external operator)\nIntegrating third party tools such as Prometheus for Azure Kuberenetes\nExposing telemtry as data sources for external third party (e.g. Log Analytics Workspaces for Grafana) as described here\nThe following picture summarizes potential Azure services/ features that might be potentially relevant:\nVariations\nA detailed configuration is not possible because the setup depends on the resources to be monitored and their capabilities. Therefore only guidelines are given to infer the right setup:\nCollection/ Storage (Monitoring Plane)\nTwo main decision must be made: (1) storage of telemetry and (2) push versus pull.\nThe number of app insights/ log analytic workspaces needs to be determined per environment. Production should be kept separate already for compliance/ resilience reasons. Dev/ test environments are rather a question mark. Subsuming dev/ test environments into a single monitoring plane is benefecial for the monitoring consumer, since he then has to check only a single place. That also means you need an additional mechanism inferring the environment for later drill down or root cause analysis. Additional custom attributes are recommended if possible. Separate App Insights/ Log Analytic instances per environment require another one for a consolidated dev/ test view.\nMicrosoft recommends a single app insights resource in the following cases as described here:\nFor application components that are deployed together. Usually developed by a single team, managed by the same set of DevOps/ITOps users.\nIf it makes sense to aggregate Key Performance Indicators (KPIs) such as response durations, failure rates in dashboard etc., across all of them by default (you can choose to segment by role name in the Metrics Explorer experience).\nIf there is no need to manage Azure role-based access control (Azure RBAC) differently between the application components.\nIf you don’t need metrics alert criteria that are different between the components.\nIf you do not need to manage continuous exports differently between the components.\nIf you do not need to manage billing/quotas differently between the components.\nIf it is okay to have an API key have the same access to data from all components. And 10 API keys are sufficient for the needs across all of them.\nIf it is okay to have the same smart detection and work item integration settings across all roles.\nStoring telemetry within the monitoring plane is easy to set up if the Azure service supports diagnostic settings or comes with app insights integration. App insights instrumentation allows extensive customization such as preprocessing. Log Analytics allows less customization out-of-the box.\nLog analytics can target cheap Azure blob storage. It can be accessed with Kusto and would also eliminate the need for archiving. However, an shared access signature is required in this case which has to be renewed. Updating a saved query is only possible for Log Analytics workspace. Due to simpler setup storing the telemetry inside the monitoring plane is the recommended option.\nPull via metrics explorer is only possible for metrics but not logs. Pushing via a custom script makes sense if:\nAPI restrictions on monitoring plane are not a problem. E.g. not being able to set the timestamp according to original occurence.\nTracking of UI driven actions that are not pushed automatically\nService targets log analytic workspace but built-in limitations like filtering/ aggregations needed before ingestions in workspace\nThe table below compares various options:\nDiagnostic Settings\nApp Insights Logging\nPush via resource API\nMetrics Explorer\nPossible per resource\n(X)\n(X)\nX\n(X)\nTelemetry Customization\nLimited\nHigh\nLimited-High\nLimited\nCustom Logging in executed code\nX\nTelemetry always captured\nX\n(X)\nX\nX\nLatency\nMedium\nMedium\nMedium\nLow\nDirection\nPush\nPush\nPush\nPull\nComments:\nOption “Push via resource API” ⇒ A scheduled script that reads periodically telemetry and pushes it to monitoring plane using the Rest API\n„Telemetry always captured“ ⇒ Some resources allow multiple ways to run something e.g. via UI or programmatically. If the telemetry is always captured the way does not matter.\nVisualization/ Alerting (Monitoring Plane)\nSee the options below for dashboarding/ visualization:\nAzure\nThird party\nWorkbooks\nDashboards\nPower BI\nGrafana\nAuto refresh in 5 Min Intervall\nX\nX\nX\nFull screen\nX\nX\nX\nTabs\nX\nX\nX\nFixed Parameter lists\nX\nX\nX\nDrill down\nX\nX\nAdditional hosting required\nX\nTerraform Support\nX\nX\nX\nRegarding components for logs/ metrics:\nMetrics: Pull (Metrics explorer) or push (Kusto query targeting data source) possible\nLogs: Push to monitoring plane only\nGrafana can be used for visualization via using a connector for log analytics workspace\nWhen to use\nThis solution assumes that your application monitoring plane is in Azure and that your monitored resources are located in Azure.\nInfrastructure Monitoring\nOverview\nThe solution is to use Azure Monitor with Log Analytics and the following platform regarding the monitored resources. The focus of this chapter is to introduce the available features. Recommendations for a concrete setup are given in the next chapter.\nThe relevant Azure monitor features are as follows:\nData Sources/ Instrumention\nA major source for infrastructure is the health information provided by the platform. The following health information is relevant:\nService Health Information which also includes planned downtime of the Azure platform and problems on service type level such as VMs\nResource Health which includes health information for service instances you created\nOn resource level resource utilization is relevant. This includes:\nHitting capacity limits regarding CPU/ memory\nIdle resources\nAvailability differs per service. They are usually exposed via metrics.\nCollection/ Storage (Monitoring Plane)\nTelemetry can either be stored internally inside the monitoring plane or externally.\nTelemetry can be pulled from the monitoring plane. This is limited to metrics but faster than pushing. Pushing can be necessary if the telemetry is not available in Azure monitor out of the box or pulling from the monitored resources is not possible. Pushing can be done as follows:\nResource diagnostic: Useful to push resource specific telemtry.\nHealth diagnostic: Resource Health tracks the health of your resources for specific known issues. With diagnostic settings configured on subscription level you can send that data to Log Analytics workspace. You will need to send the ResourceHealth/ Service Health categories (Source Health-Overall Source Possible-Categories).\nAnalysis/ Diagnosis (Monitoring Plane)\nHealth relevant KPIs can be determined via Kusto as shown in the example below:\nAzureActivity\n// Filter only on resource health data in activity log\n| where CategoryValue == 'ResourceHealth'\n// dump any resource health data where the health issue was resolved. We are interested only on unhealthy data\n| where ActivityStatusValue &lt;&gt; \"Resolved\"\n// Column Properties has nested columns which we are parsing as json\n| extend p = parse_json(Properties)\n// Column the parsed Properties column is now a dynamic in column p\n// We take the top level properties of column p and place them in their own columns that start with prefix Properties_\n| evaluate bag_unpack(p, 'Properties_')\n// We do the same for the newly created column Properties_eventProperties\n| extend ep = parse_json(Properties_eventProperties)\n| evaluate bag_unpack(ep, 'EventProperties_' )\n// We list the unique values for column EventProperties_cause\n| distinct EventProperties_cause\nAvailability of resource utilization specific KPIs depends on the monitored resources.\nKusto queries across multiple application insights or log analytic workspaces are possible (See app monitoring for details).\nLog Analytics comes with the following tools for exploration and root cause analysis:\nTable based access allows you to define different permissions per log table. This is done using custom roles where you define the tables as part of the resource type as described here.\nAdditional management solutions: They have to be installed per werkspace. An example is the ITSM Connector used to automatically create incidents or work items when Alerts are created within Log Analytics. Such as System Center Service Manager or Service Now.\nLog analytics agent managentment: agent collects telemetry from Windows and Linux virtual machines in any cloud, on-premises machines, and those monitored by System Center Operations Manager and sends it collected data to your Log Analytics workspace in Azure Monitor. The Log Analytics agent also supports insights and other services in Azure Monitor such as VM insights, Azure Security Center, and Azure Automation as described here.\nService Map automatically discovers application components on Windows and Linux systems and maps the communication between services. Service Map shows connections between servers, processes, inbound and outbound connection latency, and ports across any TCP-connected architecture, with no configuration required other than the installation of an agent as described here.\nVisualization/ Alerting (Monitoring Plane)\nSee Application monitoring features for alerts and visualization.\nThe following picture summarizes potential Azure services/ features that might be potentially relevant:\nVariations\nSee application monitoring.\nWhen to use\nThis solution assumes that your infrastructure monitoring plane is in Azure and that your monitored resources are located in Azure.\nAbout\nAbout devonfw\nFeatures\nTechnology Stack\nExplore\nGetting started\nArchitecture\nResources\nDocs\nUser guide\nReleases information\nWiki\nCommunity\nContributors\nWebsite Contribution\nTerms of Use\nSupport\nimport { UtilsModule } from '/website/shared/utils.js';\nimport { HeaderModule } from '/website/components/header/header.js';\nlet searchData = { index: null, documents: null };\nUtilsModule.loadIndex(searchData);\n$(window).on('load', function() {\nconst queryFun = () => {\nHeaderModule.queryFunction(searchData);\n};\nHeaderModule.searchOnClick(queryFun);\n});\nlet bb = document.getElementById('menu-button');\nbb.addEventListener('click', function() {\ndocument.querySelector('.website-navbar ul').classList.toggle('visible');\nconsole.log(document.querySelector('.website-navbar ul'))\n})\nimport { EditLinksModule } from '/website/shared/editlinks.js';\nlet alwaysVisible = true;\nif(document.location.pathname.endsWith(\"pages/welcome/welcome.html\")) {\nalwaysVisible = false;\n}\nEditLinksModule.addEditLinks(alwaysVisible);\nLast updated 2021-10-05 08:39:57 UTC\n"},{"dirname":"monitoring_openTelemetry","id":7,"path":"target/generated-docs/solutions/monitoring_openTelemetry/index.html","type":"solution","title":"Monitoring your microservices with openTelemetry","body":"\nMonitoring your microservices with openTelemetry\nWhere is the problem?\nWith the complexity and size of microservice systems, which include hundreds of small services, keeping an overview can be quite challenging. Usually one monolitic application can be scanned for potential errors, as failures in the system directly reference this one application. With microservices the area to search in could be narrowed down, but never to an extend adressing one specific service being the root cause. Therefore, to keep track of runtime problems and the fullfillment of nonfunctional reqirements corresponding to specififc microservices advanced monitoring needs emerge.\nThere are different proposals for collecting this telemetry data consisting of logs, traces and metrics with each proposal having their own benefits and disadvantages leading to a heterogenous landscape of these solutions special for each microservice. As these are configured on their own and little to no replacability options without nudging big change efforts are given, flexibility is limited blocking possibly better solutions. Concluding, overall and in the microservice teams there is not a central repository for looking up your microservices telemetry data but again a numerous amount of different hardly replacable solutions for different microservices making it difficult to gain a central overview e.g. via Grafana.\nThe value for the customer\nMonitoring in microservice settings generally benefits nonfunctional requirements such as performance, availability or security through transparent insights, while enabling proactive but also reactive handling of issues. By having all the telemetry data centrally stored and providing exchangeable solutions for the different types of data, no restrictions to design decisions is made and the customer can gain a fast overview over a great amount of microservices.\nIntroducing OpenTelemetry\nOpenTelemetry forms the combination of OpenTracing and OpenCensus collecting different telemetry data (metrics, logs, traces) for understanding the applications behavior and performance. Because of the standardization for processing telemetry data, OpenTelemetry acts as a central collector for whole application landscapes monitoring data, which is able to communicate with replacable logging-, tracing- or metrics-backends without changing configurations in the applications code.\nAddressing your business\nOpenTelemetry can be used in various use cases, but is best suited in the microservice context. Therefore, any business apllication relying on microservices would be a suited domain for applying an OpenTelemetry solution.\nOpenTelemetry architecture\nThe above example shows a reference architecture of multiple hosts (environments/applications).\nTraces and logs are automatically collected at each JAX-RS service. Other additionally defined metrics, traces or logs are also collected.\nThese applications send data directly to a otel-agent configured to use fewer resources. An otel-agent is a collector instance running with the application or on the same host as the application.\nThe agent then forwards the data to a collector that receives data from multiple agents. Collectors on this layer typically are allowed to use more resources and queue more data.\nThe collector then sends the data to the appropriate backend, in the solution provided by Jaeger, Zipkin or VictoriaMetrics. The Victoria backend serves metrics while Jaeger and Zipkin are two alternatives for tracing._\nAdditionally all the telemetry data can be visualized in a tool like Grafana.\nThe OpenTelemetry collector is an instance that makes it possible to receive telemetry data, optionally transform it and send the data on. Receiving, transforming and sending data is done via pipelines. A pipeline consists of a set of receivers, a set of optional processors and a set of exporters.\nA reveiver transfers the telemetry data into the collector, which then can be processed on a processor. Finally, an exporter can send the data to a corresponding backend/destination. Further information can be found here\nList of available receivers, processors and exporters:\nreveivers\nprocessors\nexporters\nIn addition, extensions (Health Check, Performance Profiler, zPages) are provided that can be added to the collector to extend the primary functionality of the collector. These do not require direct access to telemetry data and enable additional functionality outside the usual pipeline.\nRelated documentations\nOpenTelemetry Collector\nOpenTelemetry Java documentation\nJaeger\nZipkin\nVictoriaMetrics\nGrafana\nRelated Architectures and Alternatives\nopenTracing\nopenCensus\n"},{"dirname":"monitoring_platforms","id":8,"path":"target/generated-docs/solutions/monitoring_platforms/index.html","type":"solution","title":"Monitoring Platforms","body":"\nTable of Contents\nMonitoring Platforms\nAzure\nMonitoring Platforms\nAzure\nThis chapter lists major features/ concrete services for monitoring of the Azure platform. This architecture pattern builds on the general problem description for monitoring. A detailed discussion of services is part of the solution design in the subsequent chapters. Major features per stage of the monitoring pipeline are as follows:\nData Sources/ Instrumention\nTelemetry in Azure is split in logs and metrics. Logs contain non-structured text entries whereas metric is a value measured at a certain time. Dimensions are additional characterisitics of the measured metric.\nThe major logs/ metrics are one of the following categories: (1) Activity logs, (2) resource logs (former diagnostic logs) and (3) Azure Active Directory (=AAD) related logs. Activity logs track actions on Azure Resource Manager level such as creation, update or deletion of Azure resources. Resource logs track operations within a resource such as reading secrets from a key vault after it has been created.\nMonitoring Plane\nThe services used for processing depend on the perspective. A major stop for a unified end-to-end monitoring is Azure Monitor. It unifies the former separate services Application Insights and Log Analytics as features. Application Insights is focusing at application monitoring whereas Log Analytics started as part of the operation management suite targeting infrastructure monitoring. Both come with their own repository for storing the telemetry. In the future a Log Analytic Workspace will be the central place for collecting data from infrastructure and application perspective.\nTelemetry can either be (1) forwarded (=pushed) to the monitoring plane or (2) pulled from the monitoring plane.\nPushing can be necessary if the telemetry is not available in Azure monitor out of the box or pulling from the monitored resources is not possible. Monitored resources have to be instrumented to forward telemetry to the monitoring consumer for later processing within the monitoring plane. App insight requires linking via instruentation keys. Log Analytic workspaces require diagnostic settings. Possible targets are only log analytics workspace, event hub or azure blob storage. Telemetry that can be forwarded is predefined. Fine granular selection of metrics/ logs is not always possible.\nPulling reads telemetry such as metrics directly from the monitored resource. Logs cannot be read directly and require pushing. Compared to pushing this method is also faster.\nBoth features cover health and performance perspectives. Cost management is covered by Azure Cost Management. The major services for monitoring compliance are Azure Security Center and Azure Sentinel (Larger enterprise scope compared to Azure Security Center with SIEM and SOAR capabilities).\nAzure monitor provides various options for visualizations but also other services are possible. Dashboards like features provide a single pane of control across a number resources. Kusto is the major language for analyzing logs and metrics e.g. as part of the root cause analysis. Additional features of app insights/ log analytics complement the language.\nAlert thresholds can be dynamic and actions can be grouped in action groups for multiple reuse. Dynamic Thresholds continuously learns the data of the metric series and tries to model it using a set of algorithms and methods as described here. Alerts can be grouped dynamically to reduce noise and filtered/ scoped to reduce false alarms.\nVarious options for archiving exist in Azure such as Logic Apps. A cheap archive is usually Azure blob storage. Policies can be used to automatically delete archived blobs. Removal of ingested telemetry is configurable by setting the retention period accordingly in Log Analytics/ App Insights.\nImproving Feedback Loop (Plane/ Resources)\nThe platform allows to track track end-user behavior and engagement. Impact Analysis helps to prioritize which areas to focus on to improve the most important KPIs as described here. Autoscaling is provided by Azure monitor and other Azure services directly.\nAzure monitor can integrate with and forward telemetry from various sources. Some services like Azure Security center forward telemtry to Azure monitor.\nIT service management tools such as ServiceNow or System Center Service Manager can integrate with Azure monitoring tools.\nAzure provides the standard compliance mechanisms also for monitoring which ensure authentication/ authorization (via Azure Active Directory), compliance for data at-rest and in-transit.\nAbout\nAbout devonfw\nFeatures\nTechnology Stack\nExplore\nGetting started\nArchitecture\nResources\nDocs\nUser guide\nReleases information\nWiki\nCommunity\nContributors\nWebsite Contribution\nTerms of Use\nSupport\nimport { UtilsModule } from '/website/shared/utils.js';\nimport { HeaderModule } from '/website/components/header/header.js';\nlet searchData = { index: null, documents: null };\nUtilsModule.loadIndex(searchData);\n$(window).on('load', function() {\nconst queryFun = () => {\nHeaderModule.queryFunction(searchData);\n};\nHeaderModule.searchOnClick(queryFun);\n});\nlet bb = document.getElementById('menu-button');\nbb.addEventListener('click', function() {\ndocument.querySelector('.website-navbar ul').classList.toggle('visible');\nconsole.log(document.querySelector('.website-navbar ul'))\n})\nimport { EditLinksModule } from '/website/shared/editlinks.js';\nlet alwaysVisible = true;\nif(document.location.pathname.endsWith(\"pages/welcome/welcome.html\")) {\nalwaysVisible = false;\n}\nEditLinksModule.addEditLinks(alwaysVisible);\nLast updated 2021-10-05 08:40:01 UTC\n"},{"dirname":"monitoring_problem","id":9,"path":"target/generated-docs/solutions/monitoring_problem/index.html","type":"solution","title":"Monitoring","body":"\nTable of Contents\nMonitoring\nContext &amp; Problem\nMonitoring\nContext &amp; Problem\nTerminology\nUsed definition: A monitoring solution helps the monitoring consumer achieve the satisfactory level of control of a defined service. (Link to source)\nThis definition already includes the following:\nDefined service: The resources you want to monitor aka monitored resources. The resources to be monitored can be split in infrastructure and applications on top.\nLevel of control: That is your bandwidth in which your defined service operates normally aka known as baseline\nMeasuring: A measurement is a single act that quantifies an attribute of a part, equipment, service or process (CPU load, available memory etc.). Data measured is emitted by the monitored resources and aka telemetry.\nMonitoring consumer: The user trying to keep the service within its baseline boundaries.\nThe key to achieve that is a single control plane is usually preferred to simplify the operations for the consumer aka monitoring plane. The relevant content depends on the perspective of the consumer such as performance, costs, compliance and health. Performance in this pattern includes the following as described here:\nHealth monitoring: purpose of health monitoring is to generate a snapshot of the current health of the system so that you can verify that all components of the system are functioning as expected.\nError monitoring: Bugs &amp; errors need to be detected by monitoring. Supporting information must be provided that allows monitoring consumer to analyze the root cause.\nAvailability monitoring: A truly healthy system requires that the components and subsystems that compose the system are available. Availability monitoring is closely related to health monitoring. But whereas health monitoring provides an immediate view of the current health of the system, availability monitoring is concerned with tracking the availability of the system and its components to generate statistics about the uptime of the system.\nPerformance monitoring: As the system is placed under more and more stress (by increasing the volume of users), the size of the datasets that these users access grows and the possiblity of failure of one or more components becomes more likely. Frequently, component failure is preceded by a decrease in performance. If you’re able detect such a decrease, you can take proactive steps to remedy the situation.\nSLA Monitoring: SLA monitoring is closely related to performance monitoring. But whereas performance monitoring is concerned with ensuring that the system functions optimally, SLA monitoring is governed by a contractual obligation that defines what optimally actually means. You can calculate the percentage availability of a service over a period of time by using the following formula: %Availability = ((Total Time – Total Downtime) / Total Time ) * 100\nProviding a control plane requires a monitoring pipeline that should be implemented as feedback loop. The pipeline transforms raw telemetry into meaningful information that the monitoring consumer can use to determine the state of the system. The loop ensures that lessons learnt are the starting point for further improvements on the defined service side. E.g. by adaptive scaling depending on monitored traffic. The entire monitoring must be compliant and provide integration features. The conceptual stages of the pipeline are as follows:\nData Sources/ Instrumention (Monitored resources): concerned with identifying the sources from where the telemetry needs to be captured, determining which data to capture and how to capture it.\nCollection/ Storage (Monitoring plane)\nAnalysis/ Diagnosis (Monitoring plane): generate meaningful information that an monitoring consumer can use to determine the state of the system\nVisualization/ Alerting (Monitoring plane): decisions about possible actions to take and then feed the results back into the instrumentation and collection stages\nThe picture below summarizes the aspects:\nStandard Problems\nThe list below describes the standard problems that apply independent from the monitoring consumer perspective. Solutions with concrete technology are first described in subsequent chapters. Per monitoring pipeline stage the following standard problems are known:\nData Sources/ Instrumention (Monitored Resources)\nThis also includes the possibility of preprocessing to reduce or enrich sent telemtry data to the monitoring consumer. Telemetry itself might be of different structure and convey different information.\nCollection/ Storage (Monitoring Plane)\nThe drop location of the telemetry needs to be determined such as inside the monitoring plane or externally.\nMonitoring can result in a large amount of data. Storing such granular data is costly. Therefore an archiving mechanism is required to make sure costs are not exploding. Once archived the ingested telemetry should be removed.\nAnalysis/ Diagnosis (Monitoring Plane)\nIncludes standard problems like:\nFiltering\nAggregation\nCorrelation\nReformating\nComparison against Key Performance Indicatorss (=KPIs). KPIs have no weight in software development unless they are paired with your business goals. You don’t need a handful of KPI metrics for your software team. All you need is the right KPI to help you improve your product or process. KPIs should be SMART (S = Specific; M = Measureable; A = Assignable; R = Realistic; T = Time Bound). Examples: Code Quality KPIs such as Maintainability index, Complexity metrics, Depth of inheritance, Class coupling, Lines of code; Testing Quality such as Test effort, Test coverage; Availability = Mean time between failures, Mean time to recovery/ repair as described here\nVisualization/ Alerting (Monitoring Plane)\nIncludes standard problems like:\nVisualization for monitoring consumer\nAlerts: Programmatic action that free the monitoring consumer from manual intervention. It states the trigger and the action to bee executed. One challenging aspect is to minimize the number of alerts or to detect patterns behind multiple alerts. Infering a suitable thresholds can be challing especially if the threshold is not static.\nReports\nAd-hoc queries\nExploration\nImproving Feedback Loop (Plane/ Resources)\nCases where the monitored resources operated outside their baseline should be the starting point for improvements. This might mean a better tuning of alerts and intervention or system requirements.\nIntegrating and compliance affect the entire pipeline. Telemetry might have to be collected from other systems to achieve a single monitoring plane. However alerts/ notications might have to be forwarded to other systems.\nOf course a monitoring must be compliant regarding the enterprise guidelines.\nThe following patterns are not dicussed here:\nProvisioning of the monitoring plane and the monitored resourves\nFor solutions with a certain technology see the specific guides on platform and concrete service level.\nAbout\nAbout devonfw\nFeatures\nTechnology Stack\nExplore\nGetting started\nArchitecture\nResources\nDocs\nUser guide\nReleases information\nWiki\nCommunity\nContributors\nWebsite Contribution\nTerms of Use\nSupport\nimport { UtilsModule } from '/website/shared/utils.js';\nimport { HeaderModule } from '/website/components/header/header.js';\nlet searchData = { index: null, documents: null };\nUtilsModule.loadIndex(searchData);\n$(window).on('load', function() {\nconst queryFun = () => {\nHeaderModule.queryFunction(searchData);\n};\nHeaderModule.searchOnClick(queryFun);\n});\nlet bb = document.getElementById('menu-button');\nbb.addEventListener('click', function() {\ndocument.querySelector('.website-navbar ul').classList.toggle('visible');\nconsole.log(document.querySelector('.website-navbar ul'))\n})\nimport { EditLinksModule } from '/website/shared/editlinks.js';\nlet alwaysVisible = true;\nif(document.location.pathname.endsWith(\"pages/welcome/welcome.html\")) {\nalwaysVisible = false;\n}\nEditLinksModule.addEditLinks(alwaysVisible);\nLast updated 2021-10-05 08:40:03 UTC\n"},{"dirname":"provisioning_azure_devops","id":10,"path":"target/generated-docs/solutions/provisioning_azure_devops/index.html","type":"solution","title":"Provisioning Solution Azure Native","body":"\nTable of Contents\nProvisioning Solution Azure Native\nOverview\nPattern Details\nVariations\nWhen to use\nProvisioning Solution Azure Native\nOverview\nThe cloud native provisioning service in Azure is Azure DevOps formerly known as Team Foundation Server (TFS). Azure DevOps covers the full CI/ CD requiremenzs including project management. Due to its extensive extension features you can also replace features with other alternatives (see also under variations).\nNote: Azure DevOps will be superseeded by GitHub in the long run after Microsoft acquired GitHub. New features will be initially implemented there.\nThe services that (can) complement Azure DevOps:\nAzure Key Vault for storing secrets/ exchange of settings\nAzure App Configuration\nThis service provides settings (key-value pairs) and feature toggles. Native integrations exist for typical application programming languages like .NET/ Java. However native integrations with terraform do not exist and it is also not hardened for sensitive information as key vault. Therefore, it is recommended to use that service as special case for application layer if feature toggles are needed.\nAzure AD\nAzure Active Directory provides the service principal the pipelines run with.\nMonitoring\nAzure DevOps generates metrics to check the health pipelines and displays te state in the Azure DevOps portal. However no built-in forwarding to App Insights independent from the deployed application exists. The following options are available:\nContinous monitoring: Monitoring which assumes Web Applications.\nRest API to read service health information manually as explained here. Check out the pattern monitoring cloudnative how polling and forwarding to azure monitor can be achieved.\naudit streaming is similar to diagnostic settings (In preview as of 21.09.2021). It allows to configure a constant forwarding of telemetry to selected targets as described here.\nStructural elements to model environments\nThe picture illustrates the setup with the major dependencies:\nPattern Details\nGeting Started\nMany aspects influence the setup of the service. Following a top down approach the following decisions have to be made:\nDefine landing zone of the service itself in Azure (out of scope)\nOrganizational mapping\nThis yields the structural components to host provisioning which will be detailed in the next chapter. It introduces the possible components and guidelines for its structuring.\nModelling other outlined aspects across automation, infra/ app code and provisioning\nThe structural components are organizations, teams and projects. A team is a unit that supports many team-configurable tools. These tools help you plan and manage work, and make collaboration easier. Every team owns their own backlog, to create a new backlog you create a new team. By configuring teams and backlogs into a hierarchical structure, program owners can more easily track progress across teams, manage portfolios, and generate rollup data.\nA project in Azure DevOps contains the following set of features:\nBoards and backlogs for agile planning\nPipelines for continuous integration and deployment\nRepos\nThe service comes with hosted git repositories inside that service. You can also use the following external source repositories: Bitbuckt Cloud, GitHub, Any generic git repo, Subversion\nTesting\nAzure DevOps supports the following testing by defining test suites with test cases:\nPlanned manual testing. Manual testing by organizing tests into test plans and test suites by designated testers and test leads.\nUser acceptance testing. Testing carried out by designated user acceptance testers to verify the value delivered meets customer requirements, while reusing the test artifacts created by engineering teams.\nExploratory testing. Testing carried out by development teams, including developers, testers, UX teams, product owners and more, by exploring the software systems without using test plans or test suites.\nStakeholder feedback. Testing carried out by stakeholders outside the development team, such as users from marketing and sales divisions.\nTests can also be integrated in pipelines. Pipelines support a wide range of frameworks/ libraries.\nEach organization contains one or more projects\nYour business structure should act as a guide for the number of organizations, projects, and teams that you create in Azure DevOps. Each organization gets its own free tier of services (up to five users for each service type) as follows. You can use all the services, or choose just what you need to complement your existing workflows.\nAzure Pipelines: One hosted job with 1,800 minutes per month for CI/CD and one self-hosted job\nAzure Boards: Work item tracking and Kanban boards\nAzure Repos: for version control and management of source code and artifacts\nAzure Artifacts: Package management\nTesting: Continuous test integration throughout the project life cycle\nAdding multiple projects makes sense in the following cases (see azure projects):\nTo prohibit or manage access to the information contained within a project to select groups\nTo support custom work tracking processes for specific business units within your organization\nTo support entirely separate business units that have their own administrative policies and administrators\nTo support testing customization activities or adding extensions before rolling out changes to the working project\nTo support an Open Source Software (OSS) project\nAdding teams instead of projects is recommended over projects due to agile culture:\nVisibility: It’s much easier to view progress across all teams\nTracking and auditing: It’s easier to link work items and other objects for tracking and auditing purposes\nMaintainability: You minimize the maintenance of security groups and process updates.\nThe table below lists typical configurations along with their characteristics:\nCriteria\n1 project, N teams\n1 org, N projects/ teams\nN orgs\nGeneral guidance\nSmaller or larger organizations with highly aligned teams\nGood when different efforts require different processes (multi)\nLegacy migration\nProcess\nAligned processes across teams; team flexibility to customize boards, dashboards, and so on\nDifferent processes per prj;e.g. different work item types, custom fields\nsame as many projects\nRemaining goals (Automation Code)\nThis chapter details how the above conceptual features can be achieved with Azure DevOps pipelines.\nThe pipeline programming approach can be either UI driven or programmatic by using YAML. YAML organizes pipelines into a hierarchy of stages, jobs and tasks. Tasks are the workhorse where activities are implemented. Tasks support scripting languages as stated below. They in turn allow to install additional libraries frameworks from third party providers such as terraform (or you use extensions that give you additional task types). The list below highlights a few YAML points you have to be aware of:\nPassing files/ artefacts between jobs/ pipelines\nPassing between jobs within the same pipeline requires publishing the files as pipeline artefacts and downloading it afterwards. Passing between syntax requires a different syntax and also requires a version.\nVariables\nVariables can have different scopes. A special syntax is required to publish them at runtime and to consume them in a different job (requires declaration). (Link). Various predefined exist.\nObtaining client secret\nScripting languages such as terraform might require the client secret for embedded scripting blocks. However, terraform does not provide a way to get it. The only way was to include an AzureCLI scripting task. Setting the argument \"addSpnToEnvironment\" to true makes the value for scripting languages as environment variable. A script can then publish the variable so that the value is available in the YAML pipeline.\nPipelines that shall be triggered by pushing to the repo state in the trigger element the details like branch when they shall run.\nThe example below shows a scheduled trigger:\n# Disable all other triggers\npr: none\ntrigger: none\n# Define schedule\nschedules:\n# Note: Azure DevOps only understands the limited part of the cron\n# expression below. See this link for further details:\n# https://docs.microsoft.com/en-us/azure/devops/pipelines/process/scheduled-triggers?view=azure-devops&amp;tabs=yaml\n# Note: With DevOps organization setting of UTC+1 Berlin,...\n# for a given hour x you have to specify x-2 e.g. 16:00 will be\n# started 18:00 o'clock\n- cron: \"30 5 * * MON,TUE,WED,THU,FRI\"\ndisplayName: Business daily morning creation\nalways: true # also run if no code changes\nbranches:\ninclude:\n- 'refs/heads/master'\nPull request (PR) triggers cause a pipeline to run whenever a pull request is opened with one of the specified target branches, or when changes are pushed to such a pull request. In Azure Repos Git, this functionality is implemented using branch policies. To enable pull request validation in Azure Git Repos, navigate to the branch policies for the desired branch, and configure the Build validation policy for that branch. For more information, see Configure branch policies. Draft pull requests do not trigger a pipeline even if you configure a branch policy. Building pull requests from Azure Repos forks is no different from building pull requests within the same repository or project. You can create forks only within the same organization that your project is part of (see PR triggers).\nTo trigger a pipeline upon the completion of another pipeline, specify the triggering pipeline as a pipeline resource. The following example has two pipelines - app-ci (the pipeline defined by the YAML snippet), and security-lib-ci (the triggering pipeline referenced by the pipeline resource). We want the app-ci pipeline to run automatically every time a new version of security-lib-ci is built.\n# this is being defined in app-ci pipeline\nresources:\npipelines:\n- pipeline: securitylib # Name of the pipeline resource\nsource: security-lib-ci # Name of the pipeline referenced by the pipeline resource\nproject: FabrikamProject # Required only if the source pipeline is in another project\ntrigger: true # Run app-ci pipeline when any run of security-lib-ci completes\nImplicit Chaining for orchestration is possible by using trigger condition. Calling pipelines explicitly is so far only possible with scripting. The code snippet below shows an example:\n#\n# Make call to schedule pipeline run\n#\n# Body\n$body = @{\nstagesToSkip = @()\nresources = @{\nself = @{\nrefName = $branch_name\n}\n}\ntemplateParameters = $params\nvariables = @{}\n}\n$bodyJson = $body | ConvertTo-Json\n# Uri extracted from the Azure DevOps UI\n# $org_uri and $prj_id contain names of current organization/ project\n# $pl_id denotes the internal pipeline id to be started\n$uri = \"${org_uri}${prj_id}/_apis/pipelines/${pl_id}/runs?api-version=5.1-preview.1\"\n# Output paramters\nWrite-Host(\"-------- Call ${pl_name} --------\")\nWrite-Host(\"Headers: ${headersJson}\")\nWrite-Host(\"Json body: ${bodyJson}\")\nWrite-Host(\"Uri: ${uri}\")\ntry\n{\n# Trigger pipeline\n$result = Invoke-RestMethod -Method POST -Headers $headers -Uri $uri -Body $bodyJson\nWrite-Host(\"Result: ${result}\")\n# Wait until run completed\n$buildid = $result.id\n$start_time = (get-date).ToString('T')\nWrite-Host(\"------------ Loop until ${pl_name} completed --------\")\nWrite-Host(\"started runbuild ${buildid} at ${start_time}\")\n# Uri for checking state\n$uri = \"${org_uri}${prj_id}/_apis/pipelines/${pl_id}/runs/${buildid}?api-version=5.1-preview.1\"\nDo {\nStart-Sleep -Seconds 60\n$current_time = (get-date).ToString('T')\n# Retrieve current state\n$result = Invoke-RestMethod -Method GET -Headers $headers -Uri $uri\n$status = $result.state\nWrite-Host(\"Received state ${status} at ${current_time}...\")\n} until ($status -eq \"completed\")\n# return result\n$pl_run_result = $result.result\nWrite-Host(\"Result: ${pl_run_result}\")\nreturn $pl_run_result\n}\ncatch {\n$excMsg = $_.Exception.Message\nWrite-Host(\"Exception text: ${excMsg}\")\nreturn \"Failed\"\n}\nOrchestration must take dependencies into account. They might result from the deployed code or the scope of the pipeline (scope is e.g. a single microservice and code includes the libraries needed).\nOrchestrated pipelines must pass data between them. The recommended method is to use key vault.\nRecreation of resources in short intervals might cause pipelines to fail. Even if resources are deleted they might still exist in the background (even although soft delete is not applicable). Programming languages can therefore get confused if pipelines recreate things in short intervals. Creating a new resource group can solve the problem since they are part of the tecnical resource id.\nAs part of the configuration Azure DevOps provides the possibility to provide various settings that are used for development such as enforcing pull requests instead of direct pushes to the repo.\nThe major configuration mechanisms in YAML are variables, parameters and variable groups. Variable groups bundle multiple settings as key value pairs. Parameters are not possible in a variable section (Dynamic inclusion of variable groups is possible via file switching). If they are declared on top level they have to be passed when the pipeline is called programmatically or manually by the user.\nQuality gates can be enforced as follows:\nStatic code analysis:\nVarious tool support exists depending on the programming language.\nAutomated tests (Unit, Integration, End-To-End)\nTests can be included in pipelines via additional libraries and additional previous installment through scripting. The task below uses an Azure CLI task to run tests for terraform:\n- task: AzureCLI@2\ndisplayName: Run terratest\ninputs:\nazureSubscription: ${{parameters.svcConn}}\nscriptType: bash\nscriptLocation: 'inlineScript'\naddSpnToEnvironment: true\ninlineScript: |\n# Expose required settings as environment variables\n# ARM_XXX initialized by task due to addSpnToEnvironment = true\nsubsid=`az account show --query id -o tsv`\necho \"client_id:\"$servicePrincipalId\necho \"client_secret:\"$servicePrincipalKey\necho \"subscription_id:\"$subsid\necho \"tenant_id:\"$tenantId\nexport ARM_SUBSCRIPTION_ID=$subsid\nexport ARM_CLIENT_ID=$servicePrincipalId\nexport ARM_CLIENT_SECRET=$servicePrincipalKey\nexport ARM_TENANT_ID=$tenantId\n# Backend settings\nexport storage_account_name=${{parameters.bkStname}}\nexport container_name=${{parameters.bkCntName}}\nexport key=${{parameters.bkRmKeyName}}\n# Other settings\nexport resource_group_name=${{parameters.rgName}}\n# Switch to directory with tests\npwd\ncd test\n# Testfile must end with \"&lt;your name&gt;_test.go\"\ngo test -v my_test.go\nManual approval e.g. for production\nYAML allows deployments to named environments. Approvers can then be defined for the named environments in the portal what causes the deployment pipeline to wait. However Approval must be done multiple times if you have multiple deplyoment blocks. The example below shows a deployment to the environment \"env-demo\":\njobs:\n- deployment:\ndisplayName: run deploy template\npool:\nvmImage: 'ubuntu-latest'\nenvironment: env-demo\nstrategy:\nrunOnce:\ndeploy:\nsteps:\n# - 1. Download artefact\n- task: DownloadPipelineArtifact@2\ndisplayName: Get artefact\ninputs:\ndownloadPath: '$(build.artifactstagingdirectory)'\nartifact: ${{parameters.pipelineArtifactName}}\nRemaining goals (Provisioning)\nConfiguration settings can be broken down into key value pairs. As already stated key vault is the recommended place for storage. Azure App Configuration and variable groups can reference values in Key Vault. Key Value pairs must be selected in YAML based on the target environment. Switching based on the parameter value is possible by constructing filenames based on the parameter value. The resolved filenam contains then the variable group or the key value pairs. as shown below:\n(1) Main pipeline that requires switching\n...\n# Switch in the pipeline which is implemented in a shared repository\nvariables:\n- template: ./pipelines/configurations/vars-env-single-template.yaml@repo-shared\nparameters: ${{parameters.envName}}\n...\n(2) Shared: Switch to correct configuration file\n...\nparameters:\n- name: envName\ndisplayName: name of environment\ntype: string\n# Load filename with resolved parameter value\nvariables:\n- template: vars-env-def-${{parameters.envName}}-template.yaml\n(3) Shared: Configuration file vars-env-def-dev1.yaml\nvariables:\nenvNamePPE1MainScriptLocation: app/dev\nenvNamePPE1SvcLevel: Full\nenvNamePPE1BranchName: dev\nenvNamePPE1KvEnvName: $(envNameCRST)1\nAzure DevOps can integrate with various external tools. Pipelines can be called from external and allow calling external tools. Various third party tools can be manually installed or used via extensions.\nFor compliance Azure DevOps provides various settings inside Azure DevOps itself and via Azure Active Directory.\nPortal access to Boards, Repos, Pipelines, Artifacts and Test Plans can be controlled through Azure DevOps project settings (Link).\nAzure DevOps supports the following autthentication mechanisms to connect to services and resources in your organization (Link):\nOAuth to generate tokens for accessing REST APIs for Azure DevOps. The Organizations and Profiles APIs support only OAuth.\nSSH authentication to generate encryption keys for using Linux, macOS, and Windows running Git for Windows, but you can’t use Git credential managers or personal access tokens (PATs) for HTTPS authentication.\nPersonal access token (PAT) to generate tokens for:\nAccessing specific resources or activities, like builds or work items\nClients like Xcode and NuGet that require usernames and passwords as basic credentials and don’t support Microsoft account and Azure Active Directory features like multi-factor authentication\nAccessing REST APIs for Azure DevOps\nUser permissions for team members are split in access levels and project permissions inside Azure DevOps. The Basic and higher access levels support full access to all Azure Boards features. Stakeholder access provides partial support to select features, allowing users to view and modify work items without having access to all other features. Additional restrictions are possible by Azure Active Directory settings using conditional access policies and MFA. Azure DevOps honors all conditional access policies 100% for our Web flows. For third-party client flow, like using a PAT with git.exe, IP fencing policies are supported only (no support for MFA policies).\nPermissions to work with repositories can be set under project’s repositories settings which also allows to disable forks. Many forks makes it hard to keep the overview and forking allows to download code into someones private account. Azure DevOps supports creating branch protection policies, which protect the code committed to the main branches (project settings ⇒ repo ⇒ branch policies).\nCompliance affects dealing with sensitive settings. As already stated key vault is the standard service for storing them at runtime. Exports from key vault can only be decrypted in a key vault instance.\nHence, secrets can be stored in a repository in a safe way without having to store the values in plain. Using them later should be done in a safe way. This includes publishing them in a safe way and passing them from YAML to terraform by avoiding log output in plain text. Avoiding log output passing them as environment variables/ files.\nThe following repository structure shows a conceptual breakdown that covers most aspects:\n1. Infra\n1.1. Infrastructure\n1.1.1. Other landing zones\nRepresents other areas with shared functionality that are required. Examples are environments for monitoring, the environment containing Azure DevOps, Key Vault settings etc.\n1.1.2. App Environments\nRepresents the environments where application is deployed to.\n1.1.2.1. Envs\nThis level contains all infrastructure code for seting up en environment. The split between dev and non-dev leverages cost savings for less performant dev environments e.g. by picking cheaper service configurations or totally different Azure services.\n1.1.2.1.1. Dev\n1.1.2.1.2. Non-Dev\n1.1.2.1.3. Modules\nFactored out modules for shared reuse. One example is a central module to generate the name for a given module.\n1.1.2.2. Envs-Mgmt\nCaptures aspects assumed by the chosen programming language such as terraform for managing an environment. This includes for instance the backend creation code.\n1.2. Pipelines\nPipelines for automating infrastrcuture deployment.\n2. App\n2.1. Application (Black Box)\n2.2. Pipelines\nPipelines for automating app code deployment.\n3. Shared\nCaptures shared aspects between infrastructure and application code such as publishing key vault secrets for a pipeline or triggering another pipeline.\nVariations\nThe following features of Azure DevOps can be replaced with alternatives:\nRepos\nBoards for project management\nAzure Artefacts\nThe following alternatives on service level exist:\nAzure Lab Services for Dev/ Test scenarios\nKubernetes\nAzure DevSpaces (Deprecated) in favor of “Bridge-to-kubernetes” for Dev/ Test scenarios\nBridge-to-Kubernetes\nWhen to use\nAzure DevOps makes sense if you want to provision to Azure due to its tight integration into the Azure platform. The following circumstances also speak for Azure DevOps:\nYou need a mature service on enterprise level\nYou don’t need cloud agnostic pipelines e.g. due to a multi-cloud scenario\nYour code repository is not GitHub and Azure DevOps provides integrations for it (GitHub Actions are a better alternative if your code repos are already on GitHub.)\nYou need a full blown provisioning service and plan to use the integrated repos that come with Azure DevOps\nAbout\nAbout devonfw\nFeatures\nTechnology Stack\nExplore\nGetting started\nArchitecture\nResources\nDocs\nUser guide\nReleases information\nWiki\nCommunity\nContributors\nWebsite Contribution\nTerms of Use\nSupport\nimport { UtilsModule } from '/website/shared/utils.js';\nimport { HeaderModule } from '/website/components/header/header.js';\nlet searchData = { index: null, documents: null };\nUtilsModule.loadIndex(searchData);\n$(window).on('load', function() {\nconst queryFun = () => {\nHeaderModule.queryFunction(searchData);\n};\nHeaderModule.searchOnClick(queryFun);\n});\nlet bb = document.getElementById('menu-button');\nbb.addEventListener('click', function() {\ndocument.querySelector('.website-navbar ul').classList.toggle('visible');\nconsole.log(document.querySelector('.website-navbar ul'))\n})\nimport { EditLinksModule } from '/website/shared/editlinks.js';\nlet alwaysVisible = true;\nif(document.location.pathname.endsWith(\"pages/welcome/welcome.html\")) {\nalwaysVisible = false;\n}\nEditLinksModule.addEditLinks(alwaysVisible);\nLast updated 2021-10-05 08:40:08 UTC\n"},{"dirname":"provisioning_azure_github","id":11,"path":"target/generated-docs/solutions/provisioning_azure_github/index.html","type":"solution","title":"Provisioning Solution GitHub Actions","body":"\nTable of Contents\nProvisioning Solution GitHub Actions\nOverview\nPattern Details\nVariations\nWhen to use\nProvisioning Solution GitHub Actions\nOverview\nGitHub was recently acquired by Microsoft. Microsoft announced that new features will be first incorporated into GitHub instead of Azure DevOps. This announcements shows the future strategic shift from Microsoft away from Azure DevOps to GitHub. The subset of features for implementing pipelines is called GitHub Actions. A pipeline is also called a workflow. So far GitHub Actions are limited to repositories within GitHub.\nAzure Key Vault for storing secrets/ exchange of settings\nKey vault secrets can be included by importing them as GitHub secrets as described here.\nAzure App Configuration\nThis service provides settings (key-value pairs) and feature toggles. Native integrations exist for typical application programming languages like .NET/ Java. However native integrations with terraform do not exist and it is also not hardened for sensitive information as key vault. Therefore, it is recommended to use that service as special case for application layer if feature toggles are needed.\nChanges at in the repo can be automatically pushed to an App Configuration instance as described here.\nAzure AD\nAzure Active Directory provides the service principal the pipelines run with. The essential values are stored as GitHub secrets. From there you can use them as in your workflows as described here.\nMonitoring\nGitHub Actions generates metrics to check the health pipelines and displays te state in the Azure DevOps portal. GitHub Actions provides the Checks API to output statuses, results, and logs for a workflow which you can use for download as described here. An azure service without much overhead would be Azure Automation which allows to store and run scripts for polling.\nStructural elements to model environments\nThe picture illustrates the setup with the major dependencies:\nPattern Details\nGeting Started\nMany aspects influence the setup of the service. Following a top down approach the following decisions have to be made:\nOrganizational mapping\nThis yields the structural components to host provisioning which will be detailed in the next chapter. It introduces the possible components and guidelines for its structuring.\nModelling other outlined aspects across automation, infra/ app code and provisioning\nThe structural components are organizations, teams and projects (in public beta as of 19.09.2021).\nOrganizations are a group of two or more users that typically mirror realworld organizations. They are administered by Organization members and can contain both repositories and teams.\nA project is a customizable spreadsheet that integrates with your issues and pull requests on GitHub. You can customize the layout by filtering, sorting, and grouping your issues and PRs. You can also add custom fields to track metadata. Projects are flexible so that your team can work in the way that is best for them.\nTeams give you the ability to create groups of organization members with read, write, or admin permissions to repositories that belong to the organization. Teams are also central to many of GitHub’s collaborative features, such as team @mentions, which notify appropriate groups of people that you’d like their input or attention. Teams can be both project or subject-matter focused, and can relate to job titles or interest groups within your company as well.\nWhen setting up your GitHub Enterprise instance, the immediate instinct may be to create an Organization for every project or department at your company, leading to many divided groups that function in GitHub as siloes. This may seem like a good way to manage permissions and reduce noise, but it’s not always the ideal strategy. In fact, it is detrimental to cross-team collaboration and can result in administrative headaches down the line. Common setups are shown below:\nSetup Type\nCompany size\nBenefits\nSingle Org and team\nsmall, potentially medium\nideal for highly collaborative start up\nSinge Org, multiple teams\nmedium/ small companies with strict security needs\nmore granular repo access\nMultiple Orgs and multiple teams\nLarge companys with strict repo access\nhigher level of separation, best for companys with &gt;500 users\nRemaining goals (Automation Code)\nThis chapter details how the above conceptual features can be achieved with Azure DevOps pipelines.\nGithub only allows a programming approach for pipelines. YAML organizes pipelines into a hierarchy jobs and steps (a step is also refered to as action). Steps are the workhorse where activities are implemented. Steps support scripting languages as stated below. They in turn allow to install additional libraries frameworks from third party providers such as terraform (or you use extensions that give you additional task types). The list below highlights a few YAML points you have to be aware of:\nPassing files/ artefacts between jobs/ pipelines\nArtefacts can be passed between pipelines by uploading and downloading them from GitHub as shown here.\nVariables\nVariables can have different scopes. To pass variables between steps you two ways are possble (set-env, set as output). Direct support for enum like variables do not yet exist or workarounds are needed as described here.\nObtaining client secret\nScripting languages such as terraform might require the client secret for embedded scripting blocks. Due to the direct encoding as GitHub secrets this is not a problem.\nTriggers\nYou can configure your workflows to run when specific activity on GitHub happens, at a scheduled time, or when an event outside of GitHub occurs as described\nhere.\nImplicit Chaining for orchestration is possible by using trigger condition. Calling other workflows explicitly is so far only possible with scripting as shown here here.\nOrchestrated pipelines must pass data between them. The recommended method is to use key vault.\nRecreation of resources in short intervals might cause pipelines to fail. Even if resources are deleted they might still exist in the background (even although soft delete is not applicable). Programming languages can therefore get confused if pipelines recreate things in short intervals. Creating a new resource group can solve the problem since they are part of the tecnical resource id.\nAs part of the configuration GitHub Actions provide the following configuration mechanisms:\nWorkflow input parameters\nParameters and variables can be simple key value pairs only. Values can be string and other basic types such as bool. Strings can contain structured json to more complex structures as shown below:\non:\npush:\njobs:\nbuild:\nruns-on: ubuntu-latest\nsteps:\n- name: set output\nid: set\nrun: |\necho ::set-output name=json_var::'[{ \"name\": \"test\", \"client_payload\": \"111\" }, { \"name\": \"test2\", \"client_payload\": \"222\" }] '\n- name: use output\nrun: |\necho $json_var | jq '.[].name'\nenv:\njson_var: ${{ steps.set.outputs.json_var}}\nInput parameters refer to the input parameters of a workflow.\nAction can use variables as input. Outputs (=string) of a step/ job can be used in subsequent steps/ jobs.\nEnvironments\nEnvironments can hold with protection rules such as manual approval and secrets. A workflow job can reference an environment to use the environment’s protection rules and secrets. The environment name can be set dynamically in scripts as shwon here.\nGitHub Actions includes a collection of variables called contexts and a similar collection of variables called default environment variables.\nDefault environment variables exist only on the runner that is executing your job.\nMost contexts you can use at any point in your workflow, including when default environment variables would be unavailable. For example, you can use contexts with expressions to perform initial processing before the job is routed to a runner for execution; this allows you to use a context with the conditional if keyword to determine whether a step should run.\nSecrets are encrypted environment variables that you create in an organization, repository, or repository environment. The secrets that you create are available to use in GitHub Actions workflows.\nQuality gates can be enforced as follows:\nStatic code analysis:\nVarious tool support exists depending on the programming language such as SonarCube.\nAutomated tests (Unit, Integration, End-To-End)\nTests can be included in pipelines via additional libraries and additional previous installment through scripting. The workflow below runs npm tests:\njobs:\nbuild:\nruns-on: ubuntu-latest\nsteps:\n- name: Check out code\n- uses: actions/checkout@v2\n- name: Set up node\nuses: actions/setup-node@v1\n- name: Install dependencies\nrun: npm install\n- name: Run tests\nrun: npm test\nManual approval e.g. for production\nGitHub actions allows deployments to named environments. Approvers can then be added as environments protection rules. The terraform apply command below is bound to the environment production:\nterraformapply:\nname: 'Terraform Apply'\nneeds: [terraform]\nruns-on: ubuntu-latest\nenvironment: production\nRemaining goals (Provisioning)\nGitHUb Actions can integrate with various external tools. Pipelines can be called from external (see here) and allow calling external tools. Various third party tools can be manually installed or used via extensions.\nFor compliance GitHub provides various settings as described here.\nSecrets can be configured at the organization, repository, or environment level, and allow you to store sensitive information in GitHub. They should not contain structured content like JSON since they are reacted to avoid display in logs.\nYou can use the CODEOWNERS feature to control how changes are made to your workflow files. For example, if all your workflow files are stored in .github/workflows, you can add this directory to the code owners list, so that any proposed changes to these files will first require approval from a designated reviewer.\nYou should ensure that untrusted input does not flow directly into workflows, actions, API calls, or anywhere else where they could be interpreted as executable code. In addition, there are other less obvious sources of potentially untrusted input, such as branch names and email addresses, which can be quite flexible in terms of their permitted content. For example, zzz\";echo${IFS}\"hello\";# would be a valid branch name. A pull request with title of a\"; ls $GITHUB_WORKSPACE\" would for instance list the directory if the workflow would be as follows:\n- name: Check PR title\nrun: |\ntitle=\"${{ github.event.pull_request.title }}\"\nif [[index.asciidoc_ $title =~ ^octocat ]]; then\necho \"PR title starts with 'octocat'\"\nexit 0\nelse\necho \"PR title did not start with 'octocat'\"\nexit 1\nfi\nTo help you manage the risk of dangerous patterns as early as possible in the development lifecycle, the GitHub Security Lab has developed CodeQL queries that repository owners can integrate into their CI/CD pipelines.\nActions can use the GITHUB_TOKEN by accessing it from the github.token context. It’s good security practice to set the default permission for the GITHUB_TOKEN to read access only for repository contents.\nThe following repository structure shows a conceptual breakdown that covers most aspects:\n1. Infra\n1.1. Infrastructure\n1.1.1. Other landing zones\nRepresents other areas with shared functionality that are required. Examples are environments for monitoring, the environment containing Azure DevOps, Key Vault settings etc.\n1.1.2. App Environments\nRepresents the environments where application is deployed to.\n1.1.2.1. Envs\nThis level contains all infrastructure code for seting up en environment. The split between dev and non-dev leverages cost savings for less performant dev environments e.g. by picking cheaper service configurations or totally different Azure services.\n1.1.2.1.1. Dev\n1.1.2.1.2. Non-Dev\n1.1.2.1.3. Modules\nFactored out modules for shared reuse. One example is a central module to generate the name for a given module.\n1.1.2.2. Envs-Mgmt\nCaptures aspects assumed by the chosen programming language such as terraform for managing an environment. This includes for instance the backend creation code.\n1.2. Pipelines\nPipelines for automating infrastrcuture deployment.\n2. App\n2.1. Application (Black Box)\n2.2. Pipelines\nPipelines for automating app code deployment.\n3. Shared\nCaptures shared aspects between infrastructure and application code such as publishing key vault secrets for a pipeline or triggering another pipeline.\nVariations\nFor the following features other tools can be used:\nProject management support can be added by using other tools such as Azure DevOps.\nArtefacts can be stored also in other systems\nWhen to use\nUsing GitHub makes sense in the following scenarios:\nYou need cloud agnostic pipelines e.g. due to a multi-cloud scenario\nYour code repository is GitHub and absence of projects for project management is not a problem or can be replaced with something else such as Azure DevOps\nAbout\nAbout devonfw\nFeatures\nTechnology Stack\nExplore\nGetting started\nArchitecture\nResources\nDocs\nUser guide\nReleases information\nWiki\nCommunity\nContributors\nWebsite Contribution\nTerms of Use\nSupport\nimport { UtilsModule } from '/website/shared/utils.js';\nimport { HeaderModule } from '/website/components/header/header.js';\nlet searchData = { index: null, documents: null };\nUtilsModule.loadIndex(searchData);\n$(window).on('load', function() {\nconst queryFun = () => {\nHeaderModule.queryFunction(searchData);\n};\nHeaderModule.searchOnClick(queryFun);\n});\nlet bb = document.getElementById('menu-button');\nbb.addEventListener('click', function() {\ndocument.querySelector('.website-navbar ul').classList.toggle('visible');\nconsole.log(document.querySelector('.website-navbar ul'))\n})\nimport { EditLinksModule } from '/website/shared/editlinks.js';\nlet alwaysVisible = true;\nif(document.location.pathname.endsWith(\"pages/welcome/welcome.html\")) {\nalwaysVisible = false;\n}\nEditLinksModule.addEditLinks(alwaysVisible);\nLast updated 2021-10-05 08:40:11 UTC\n"},{"dirname":"provisioning_platforms","id":12,"path":"target/generated-docs/solutions/provisioning_platforms/index.html","type":"solution","title":"Provisioning Platforms","body":"\nTable of Contents\nProvisioning Platforms\nAzure\nProvisioning Platforms\nAzure\nAutomation code\nModelling Environments\nAzure provides the following structural elements to model an environment:\nResource groups: Smallest possible container\nSubscriptions: One subscription can contain many resource groups. With a subscription discounts for dev/ test environments are possible.\nManagement groups: One management group can contain many subscriptions. They can be used enforce policies across subscriptions if environments share common characteristics.\nAn environment can be linked to another environment. Linking an environment to multiple environment is beneficial for addressing cross concerns such as monitoring (Similar to Hub/ Spoke topology in networks).\nPipeline Implementation\nThe programming approach can be either UI driven or programmatic. Pipeline programming languages such as YAML structure the actions to be performed by the pipeline and provide basic mechansims such as downloading code from the repo, parameter handling, stating triggers and triggering other programming languages. These other languages are then used to setup infrastructure such as terraform or deploying application code.\nAzure allows to trigger pipelines upon:\na push to repo\na pull request to repo\na schedule\na pipeline completion\nThe platform allows to pass parameters by various mechanisms to pipelines(Explicit per user input, programmatically). Parameters can be passed by group identifier or explicitly as key value pairs. Complex structured objects as known from object programming languages are not directly possible (Require parsing of files with object structure). Parametrization might be constrained by the used service in certain areas.\nThe platform provides support for quality gates as follows:\nStatic code analysis\nMicrosoft does not provide own tools for static code analysis but allows integration of others.\nAutomated tests (Unit, Integration, End-To-End)\nMicrosoft provides services that include test management e.g. creating test suites with test cases and getting an overview about the results.\nApproval\nAzure services support approval for a certain environments and enforcing pull requests as quality gates.\nThe Azure platform provides the following basic options to store automation code:\nServices that provide repositories\nIntegration of various external code repositories\nOrchestration\nTo orchestrate pipelines the two following basic mechanisms can be used:\nImplicit Chaining\nIn that case the complete workflow is not explicitly coded in a dedicated pipeline. Pipelines are chained implicitly by triggering events. The biggest problem with that approach is the missing single pane of control. The current state in the overall workflow is for instance only implicitly given by the currently running pipeline.\nCreating a dedicated orchestration pipeline\nAn additional pipeline triggers in this scenario other pipelines acting as building blocks. Pipelines can run separately (just run the deployment) or as part of a bigger workflow (e.g. create environment from scratch).\nOrchestration must take dependencies into account. They might result from the deployed code or the scope of the pipeline (scope is e.g. a single microservice and code includes the libraries neede).\nOrchestrated pipelines must pass data between them. The recommended method is to use key vault.\nRecreation of resources in short intervals can cause pipelines to fail since the previously deleted resource still exists in the background.(Even although soft delete is not applicable). Whether Azure really deleted everything depends on the service. For instance Azure API management seemed to be affected by that problem.\nTraceability\nTraceability requires an identifier for referencing artefacts. A standard schema is a semantic version. The platform only supports partial support for number generation such as incrementing numbers. Linking the code in the repo to a certain version depends on used repository.\nInfrastructure/ Application code\nA programming language is either \"declarative\" or \"imperative\". Declarative programming languages state the target state and it is the job of the declarative programming language how to get there. The following rules are applied to achieve that:\ncreate a resource if not there\nupdate an existing resource if different properties\ndelete resource if not there\nImperative programming languages state the how. The internal delta calculation needs to be explicitly programmed here. If possible declarative programming languages are recommended due to automatic delta calculation. Typical case is infrastructure.\nTypical declarative options are shown in detail in the table below. The overall recommendation is to go for terraform. Major reasons for downvoting Bicep/ ARM:\nARM: difficult readability for humans\nBicep: Lack of support for testing based on plan and testing ecosystem since first added recently.\nTable with declarative programming language options:\nCriteria\nBicep\nARM\nTerraform\nSame syntax across clouds\n- (Azure Only)\n- (Azure Only)\n+ (multi)\nWhat if\no (no complete prop list;only display of plan; unexpected delete)\n- (not available)\n+ (plan command)\nDetection current\no (Real anaylsis but time)\n+ (Real anaylsis)\no (Statefile)\nTesting/ static analysis\no (Only via ARM)\n+ (available)\n+ (available)\nHuman Readability\n+\n-\n+\nReverse Engineering\n- (Extra ARM step + adjust)\no (adjust)\n+ (Direct via Terraformer)\nLatest features\no (No embedded fallback)\n+ (native)\no (Time lag but embedded fallback)\nThe major options for imperative programming languages are Azure CLI, Powershell (Windows) or Linux based scripting. Azure CLI is recommended as prefered choice since it works on linux and windows based VMs.\nThe created resources should follow a uniform naming schema. This requires naming to be factored out in a centralized module. Terraform supports factoring out common code in modules. However the backend must already exist and should also follow a naming convention. The recommendation is therefore to expose the common terraform module via an additional path that does not require a backend to determine the names for the azure resources representing the backend.\nProvisioning\nOrganizational Mapping\nThe provisioning must match the organizational requirements of your organization. Azure provides services to model sub units within your organization such as departments, projects and teams.\nIntegration\nPlatform allows a modular approach to outsource certain functionality to third party software such as code repository. Which parts is service specific.\nExternal tools providing pipelines can be integrated in two conceptual ways:\nTrigger automation pipelines from external: This involves the configuration of a CI pipeline in the external tool such as Jenkins and mechanism in the automation service that invokes the CI process when source code is pushed to a repository or a branch.\nRun external pipelines from within the platform: In this approach automation reaches out to an external tool to work with the results.\nConfiguration\nConfiguration for provisioning is required in various areas:\nEnvironment: E.g. name of resource group per potential target environment\nRepository: E.g. relevant repos/ branching\nPipelines: Parameters pipelines run with such as the technical user name or settings required by the built/ deployed code.\nConcrete features used for the above three points depend on the used services. A general storage for sensitive data (keys, secrets, certificates) in Azure is always Azure Key Vault.\nCompliance\nThe standard concept for role-based access controls is called RBAC in Azure. It assigns principals (humans or technical accounts) permissions for a certain resource. Regarding provisioning the following users are relevant:\nTechnical user (service principal) the pipelines are running with\nUsers for administrating the provisioning service\nAzure Active Directory is the central service in Azure that defines and controls all principals (human/ service) per tenant.\nGranularity of roles that can be granted depend on the service. The boundaries in which users exist or permissions can be assigned is also service specific.\nAbout\nAbout devonfw\nFeatures\nTechnology Stack\nExplore\nGetting started\nArchitecture\nResources\nDocs\nUser guide\nReleases information\nWiki\nCommunity\nContributors\nWebsite Contribution\nTerms of Use\nSupport\nimport { UtilsModule } from '/website/shared/utils.js';\nimport { HeaderModule } from '/website/components/header/header.js';\nlet searchData = { index: null, documents: null };\nUtilsModule.loadIndex(searchData);\n$(window).on('load', function() {\nconst queryFun = () => {\nHeaderModule.queryFunction(searchData);\n};\nHeaderModule.searchOnClick(queryFun);\n});\nlet bb = document.getElementById('menu-button');\nbb.addEventListener('click', function() {\ndocument.querySelector('.website-navbar ul').classList.toggle('visible');\nconsole.log(document.querySelector('.website-navbar ul'))\n})\nimport { EditLinksModule } from '/website/shared/editlinks.js';\nlet alwaysVisible = true;\nif(document.location.pathname.endsWith(\"pages/welcome/welcome.html\")) {\nalwaysVisible = false;\n}\nEditLinksModule.addEditLinks(alwaysVisible);\nLast updated 2021-10-05 08:40:14 UTC\n"},{"dirname":"provisioning_problem","id":13,"path":"target/generated-docs/solutions/provisioning_problem/index.html","type":"solution","title":"Provisioning","body":"\nTable of Contents\nProvisioning\nContext &amp; Problem\nStandard Problems\nProvisioning\nContext &amp; Problem\nThis document describes patterns to automate the deployment of an application to a target environment. The major players in this documentation are (1) the process as a whole (aka provisioning), (2) automation code and (3) application/ infra code to be deployed. Additionally a third aspect is cross functionalities (3) that affect the automation and the application/infra code such as compliance.\nAutomation code:\nThe automation code is about automating parts of deployment cycle. In an ideal world this covers the entire code development cycle from opening the first branch to the final deployment. Possible activities in this cycle are:\nquality gates\nbuild\ndeployment\nQuality gates refer to testing or approval. Approval might include a manual approval to deploy to sensitive environments like production or enforcing a review before the commit through a pull request. Failing a quality gate should stop the workflow to proceed. Ideally they are maximizing application code coverage and kick in as early as possible.\nFrom a timeline perspective major events that are kicking off actions from the automation code are:\nPull request/ commit (Only triggers quality gates that don’t take too long)\nBuild\nThey can be manually, scheduled (e.g. as part of a nightly build) or automatically started. Additional quality gates ensure code quality.\nDeplyoment to target environment\nA typical quality gate for sensible environments such as production are manual approvals.\nThe major construct is a pipeline that implements a certain activity or a combination. The trigger defines the condition that kick offs a pipeline. Kicking off a pipeline usually includes parameters such as the name of the target environment. The support of different triggers is essental to cover the entire lifecycle. Pipelines can be implemented using a UI driven or programatic approach.\nPipelines are built in a modular way which also adds intermediate steps such as placing the built output in a build artefact repository for later deployment. It also introduces the need to orchestrate them into larger workflows such as creating an entire environment from scratch.\nPipelines must ensure traceability of the performed actions across the entire chain including source code repo, the targeted environment or intermediate stores. This includes a versioning schema for built artefacts. Branches in source repos must be tagged to be able to reconstruct the code behind a versioned artefact.\nApplication/ infra code:\nThe base for automation code activities is the code creating or updating the infrastructure and the application on top. Focus in this documentation is the interaction between automation code and application/ infra code. This includes:\nProgramming approach (Language or UI) for deploying infrastructure and application code\nInteraction standard problems between automation code and application/ infra code\nProblems also depend on the chosen programing approach.\nNot relevant are:\nApplication code: Structuring in repo, programming language specifics and details of build mechanism\nSpecific deployment options of the platform services forming the infrastructure\nProvisioning\nThe process must work in a compliant way end-to-end. Compliance includes the general concern security and to adhere to constraints from the organization’s perspective that need to be enforced.\nProvisoning should be subject to monitoring and must be able to adapt to the organizational structure.\nConfiguration is also a cross concern that affects both sides. From the automation side this includes properties of the environment and settings of the involved automation code. The application specific settings are out of scope in this pattern description. However, provisioning must enable the application code to access settings.\nAs shown automation does not only include pipelines but also additional configuration settings. Automation code must be able to access the infra/ app code for deployment. This pattern includes guidelines regarding options to store the code and how to structure it in a repository. Application code structure is treated as black box.\nIn most cases enterprises have already existing technologies in place that might not be based on cloud of a certain provider. Integration with other third party offerings to cover functionally also from elsewhere is also important.\nThe picture below summarizes the major aspects:\nStandard Problems\nThe following standard problems will be addressed in subsequent paragraphs:\nAutomation code\nRegarding pipelines the folloqing aspects will be detailed:\nModeling environments\nPipeline Implementation (Programing approaches, Triggers, Paramterization, Store Code, Quality Gates)\nOrchestration\nTraceability (Versioning, Tagging)\nInfra/ Application code (Programming approach,\nInteraction standard problems)\nProvisioning\nOrganizational Mapping\nIntegration\nCode Repository (Automation/ Infra &amp; App Code)\nConfiguration\nCompliance\nFor the following aspectcs check out the other defined patterns:\nMonitoring infrastructure and application code\nGeneral guidelines for structuring repositories (e.g. mono vs. multi-repo)\nGeneral guidelines for defining landing zones\nAbout\nAbout devonfw\nFeatures\nTechnology Stack\nExplore\nGetting started\nArchitecture\nResources\nDocs\nUser guide\nReleases information\nWiki\nCommunity\nContributors\nWebsite Contribution\nTerms of Use\nSupport\nimport { UtilsModule } from '/website/shared/utils.js';\nimport { HeaderModule } from '/website/components/header/header.js';\nlet searchData = { index: null, documents: null };\nUtilsModule.loadIndex(searchData);\n$(window).on('load', function() {\nconst queryFun = () => {\nHeaderModule.queryFunction(searchData);\n};\nHeaderModule.searchOnClick(queryFun);\n});\nlet bb = document.getElementById('menu-button');\nbb.addEventListener('click', function() {\ndocument.querySelector('.website-navbar ul').classList.toggle('visible');\nconsole.log(document.querySelector('.website-navbar ul'))\n})\nimport { EditLinksModule } from '/website/shared/editlinks.js';\nlet alwaysVisible = true;\nif(document.location.pathname.endsWith(\"pages/welcome/welcome.html\")) {\nalwaysVisible = false;\n}\nEditLinksModule.addEditLinks(alwaysVisible);\nLast updated 2021-10-05 08:40:17 UTC\n"},{"dirname":"security_authentication","id":14,"path":"target/generated-docs/solutions/security_authentication/index.html","type":"solution","title":"Authentication","body":"\nAuthentication\nAccess control is an important aspect for the security in IT application landscapes. There are two different aspects to distinguish:\nAuthentication (Who tries to access?)\nAuthorization (Is the one accessing allowed to do what he wants to do?)\nThis part deals with the recommendations on authentication.\nYou have the following problem to be solved\nIn large IT landscapes it is a highly recommended best-practice to centralize your authentication. Implemeting the actual authentication into every application or service is therefore considered as an anti-pattern. Instead we suggest to use a central identity and access management (IAM) solution based on established products (e.g. Keycloak).\nUsing a central IAM\nWhen using a central IAM, the user is redirected to the identity provider (IdP) when trying to access the application. The IdP returns a login page where the user can log in. After confirming that the user can access the application, the IdP returns an access token that the user can use to access the application. We recommend the use JSON Web Tokens (JWT) within the authentication flow, as this is a widely used method in modern web applications and RESTful services.\nParticipants integrate with the identity provider using protocols such as OpenId Connect, SAMLv2 or WebAuthn. The original incoming request is forwarded to the actual service and the token is added as a bearer token via HTTP header according to OAuth standard. In the application, the token can be validated according to the user’s roles and groups.\nTypically, a gateway is placed in front of the IdP and applications to act as a reverse proxy for the actual service. Incoming traffic goes through the gateway and the gateway is then responsible for authentication through integration with the identity provider. In this way, multiple applications and services can be deployed without implementing integration between the service itself and the IdP. The access token can be validated only on the gateway side or additionally passed to the application for further validation.\nServices are implemented stateless and only accept requests with valid JWT from gateway. When one of your services invokes another service it simply passes on the JWT via HTTP header. This way all sub-sequent invocations happen within the context and with the permissions of the initial user.\nThe gateway should also act as a portal that integrates the UIs of your microservices so that end users do not notice which UI comes from which service, but have the user experience (UX) of a single monolithic UI.\nWhich protocol to use\nWe suggest using OIDC as the protocol for integration with the identity provider. It is easy to integrate and works well with mobile and web-based applications. OIDC uses JSON tokens and RESTful APIs to provide the authentication information. Therefore, it is a much more lightweight solution than SAML, which uses an XML and SOAP-based approach.\nValues for the customer\nall services are independent and decoupled from the actual authentication and IAM\nauthentication can be changed without touching any of your services, only changes need to be made to your gateway(s)\nin large and complex IT landscapes, there may be different requirements for authentication via different channels (e.g. to authenticate internal users via SPNEGO and external users via WebAuthn). In such a case, you can simply set up several variants of your gateway for each channel with different endpoint URLs.\nConventions\nWe recommend the following conventions:\ndefine a short but meaningful unique alphanumeric identifier for each of your services (app-id)\nestablish a clear URL scheme for accessing your apps, e.g. https://gateway.company.com/«app-id»/\nuse a cloud infrastructure platform that allows to manage an overlay network so you can configure loadbalancers or even a service-mesh mapping your service entry points to a consistent URL schema such as https://«app-id»:8443/«app-id»/\nthis way you do not need any configuration or business knowledge inside your gateway as the routing can be implemented fully generic\nuse app-id. as a prefix to all permission groups/roles specific to your service to avoid name clashing in your central IAM\nRelated documentations\ndevon4j authentication guide\ndevon4j JWT guide\nOAuth 2.0\nOpenID Connect\nKeycloak’s securing apps guide\nIAM solutions\nKeycloak\nWSO2\nGluu Server\nForgeRock\n…​\n"},{"dirname":"security_authorization","id":15,"path":"target/generated-docs/solutions/security_authorization/index.html","type":"solution","title":"Authorization","body":"\nAccess control is an important aspect for the security in IT application landscapes. There are two different aspects to distinguish: authentication and authorization.\nThis part deals with the recommendations on authorization, concretely with the general concept and convention. Details how to implement this with specific libraries, or programming-languages are described in the individual stacks of devonfw.\nAuthorization\nDefinition:\nAuthorization is the verification that an authenticated user is allowed to perform the operation he intends to invoke.\nClarification of terms:\nFor clarification we also want to give a common understanding of related terms that have no unique definition and consistent usage in the wild.\nTable 1. Security terms related to authorization\nTerm\nMeaning and comment\nPrincipal\nAn entity that can be authenticated e.g. a user, an application\nPermission\nA permission is an object that allows a principal to perform an operation in the system. This permission can be granted (give) or revoked (taken away). Sometimes people also use the term right what is actually wrong as a right (such as the right to be free) can not be revoked.\nGroup\nWe use the term group in this context for an object that contains permissions. A group may also contain other groups. Then the group represents the set of all recursively contained permissions.\nRole\nWe consider a role as a specific form of group that also contains permissions. A role identifies a specific function of a principal. A user can act in a role.For simple scenarios a principal has a single role associated. In more complex situations a principal can have multiple roles but has only one active role at a time that he can choose out of his assigned roles. For KISS it is sometimes sufficient to avoid this by creating multiple accounts for the few users with multiple roles. Otherwise at least avoid switching roles at run-time in clients as this may cause problems with related states. Simply restart the client with the new role as parameter in case the user wants to switch his role.\nAccess Control\nAny permission, group, role, etc., which declares a control for access management.\nSuggestions on the access model\nFor the access model we give the following suggestions:\nEach Access Control (permission, group, role, …​) is uniquely identified by a human readable string.\nWe create a unique permission for each use-case.\nWe define groups that combine permissions to typical and useful sets for the users.\nWe define roles as specific groups as required by our business demands.\nWe allow to associate users with a list of Access Controls.\nFor authorization of an implemented use case we determine the required permission. Furthermore, we determine the current user and verify that the required permission is contained in the tree spanned by all his associated Access Controls. If the user does not have the permission we throw a security exception and thus abort the operation and transaction.\nWe avoid negative permissions, that is a user has no permission by default and only those granted to him explicitly give him additional permission for specific things. Permissions granted can not be reduced by other permissions.\nTechnically we consider permissions as a secret of the application. Administrators shall not fiddle with individual permissions but grant them via groups. So the access management provides a list of strings identifying the Access Controls of a user. The individual application itself contains these Access Controls in a structured way, whereas each group forms a permission tree.\nDo not use the pattern that defines non-configured permission as no limitation or in other word all permissions.\n[DB1,DB2] → allow to access DB1 and DB2\n[] → have no permission at all → good\n[] → have all permissions → bad\nNaming conventions\nAs stated above each Access Control is uniquely identified by a human readable string. This string should follow the naming convention:\n«app-id».«local-name»\nFor Access Control Permissions the «local-name» again follows the convention:\n«verb»«object»\nThe segments are defined by the following table:\nTable 2. Segments of Access Control Permission ID\nSegment\nDescription\nExample\n«app-id»\nIs a unique technical but human readable string of the application (or microservice). It shall not contain special characters and especially no dot or whitespace. We recommend to use lower-train-case-ascii-syntax. The identity and access management should be organized on enterprise level rather than application level. Therefore permissions of different apps might easily clash (e.g. two apps might both define a group ReadMasterData but some user shall get this group for only one of these two apps). Using the «app-id». prefix is a simple but powerful namespacing concept that allows you to scale and grow. You may also reserve specific «app-id»s for cross-cutting concerns that do not actually reflect a single app e.g to grant access to a geographic region.\nshop\n«verb»\nThe action that is to be performed on «object». We use Find for searching and reading data. Save shall be used both for create and update. Only if you really have demands to separate these two you may use Create in addition to Save. Finally, Delete is used for deletions. For non CRUD actions you are free to use additional verbs such as Approve or Reject.\nFind\n«object»\nThe affected object or entity. Shall be named according to your data-model\nProduct\nSo as an example shop.FindProduct will reflect the permission to search and retrieve a Product in the shop application. The group shop.ReadMasterData may combine all permissions to read master-data from the shop. However, also a group shop.Admin may exist for the Admin role of the shop application. Here the «local-name» is Admin that does not follow the «verb»«object» schema.\nData permissions\nIn some projects there are demands for permissions and authorization that is dependent on the processed data. E.g. a user may only be allowed to read or write data for a specific region. This is adding some additional complexity to your authorization. If you can avoid this it is always best to keep things simple. However, in various cases this is a requirement. Please clarify the following questions before you make your decisions how to design your access controls:\nDo you need to separate data-permissions independent of the functional permissions? E.g. may it be required to express that a user can read data from the countries ES and PL but is only permitted to modify data from PL? In such case a single assignment of \"country-permissions\" to users is insufficient.\nDo you want to grant data-permissions individually for each application (higher flexibility and complexity) or for the entire application landscape (simplicity, better maintenance for administrators)? In case of the first approach you would rather have access controls like app1.country.GB and app2.country.GB.\nDo your data-permissions depend on objects that can be created dynamically inside your application?\nIf you want to grant data-permissions on other business objects (entities), how do you want to reference them (primary keys, business keys, etc.)? What reference is most stable? Which is most readable?\nIf data-permission is the way to go, please checkout our guidance and patterns how to solve this properly.\nImplementation hints\nAccess control\nData permission\n"}]